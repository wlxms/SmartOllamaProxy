# smart_ollama_proxy.py
import sys
import io
import logging

# é…ç½® logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("smart_ollama_proxy")

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆé¿å…æ–‡ä»¶å¥æŸ„å…³é—­é—®é¢˜ï¼‰
if sys.stdout.encoding.lower() != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import httpx
from utils import json
import asyncio
from typing import Optional, List

app = FastAPI(title="Smart Ollama-DeepSeek Router")

# ============ é…ç½®åŒº ============
DEEPSEEK_API_KEY = "sk-d55c91e9576f4868adced78f7b80e098"  # åŠ¡å¿…æ›¿æ¢ï¼
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# æœ¬åœ°çœŸå® Ollama æœåŠ¡çš„åœ°å€ (é»˜è®¤æƒ…å†µä¸‹å°±æ˜¯æœ¬æœºçš„11434ç«¯å£)
LOCAL_OLLAMA_BASE_URL = "http://localhost:11434"

# ä»£ç†è‡ªèº«ç›‘å¬çš„ç«¯å£ (å¯ä»¥è‡ªå®šä¹‰ï¼Œé¿å…ä¸çœŸå®Ollamaå†²çª)
PROXY_PORT = 11435  # ä¾‹å¦‚ï¼Œè®©ä»£ç†è¿è¡Œåœ¨11435ç«¯å£

# DeepSeek è™šæ‹Ÿæ¨¡å‹é…ç½®
# è¿™é‡Œçš„åç§°å°±æ˜¯ Copilot ä¸­ä¼šçœ‹åˆ°çš„æ¨¡å‹åç§°ï¼Œä¹Ÿæ˜¯è·¯ç”±åˆ¤æ–­çš„ä¾æ®
VIRTUAL_DEEPSEEK_MODELS = [
    {
        "name": "deepseek-chat",
        "description": "DeepSeek Chat (via API)",
        "backend": "deepseek",  # è·¯ç”±æ ‡è¯†
        "actual_model": "deepseek-chat"  # å®é™…ä¼ é€’ç»™DeepSeek APIçš„æ¨¡å‹å
    },
    {
        "name": "deepseek-coder",
        "description": "DeepSeek Coder (via API)",
        "backend": "deepseek",
        "actual_model": "deepseek-coder"
    }
]
# ===============================

# å­˜å‚¨çœŸå®çš„æœ¬åœ°æ¨¡å‹åˆ—è¡¨ (å¯åŠ¨æ—¶è·å–ï¼Œå¹¶å®šæœŸæ›´æ–°)
local_models_cache = []

class OllamaGenerateRequest(BaseModel):
    model: str
    prompt: str
    stream: bool = False
    options: dict = {}

async def fetch_local_models():
    """ä»æœ¬åœ°çœŸå®çš„ Ollama æœåŠ¡è·å–æ¨¡å‹åˆ—è¡¨"""
    global local_models_cache
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            resp = await client.get(f"{LOCAL_OLLAMA_BASE_URL}/api/tags")
            if resp.status_code == 200:
                data = resp.json()
                logger.info(f"ä» Ollama è·å–çš„åŸå§‹æ•°æ®: {data}")
                local_models_cache = data.get("models", [])
                logger.info(f"å·²è·å–æœ¬åœ° Ollama æ¨¡å‹åˆ—è¡¨ï¼Œå…± {len(local_models_cache)} ä¸ªæ¨¡å‹")
            else:
                logger.warning("æ— æ³•ä»æœ¬åœ° Ollama è·å–æ¨¡å‹åˆ—è¡¨")
                local_models_cache = []
    except Exception as e:
        logger.error(f"è¿æ¥æœ¬åœ° Ollama å¤±è´¥: {e}")
        local_models_cache = []

async def periodic_model_update():
    """å‘¨æœŸæ€§æ›´æ–°æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    while True:
        await asyncio.sleep(60)  # æ¯60ç§’æ›´æ–°ä¸€æ¬¡
        await fetch_local_models()

def get_virtual_model_info(model_name: str) -> Optional[dict]:
    """æ£€æŸ¥è¯·æ±‚çš„æ¨¡å‹æ˜¯å¦ä¸ºè™šæ‹Ÿçš„ DeepSeek æ¨¡å‹"""
    for vm in VIRTUAL_DEEPSEEK_MODELS:
        if vm["name"] == model_name:
            return vm
    return None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶è·å–ä¸€æ¬¡æœ¬åœ°æ¨¡å‹åˆ—è¡¨ï¼Œå¹¶å¯åŠ¨å®šæœŸæ›´æ–°ä»»åŠ¡"""
    logger.info(f"æ™ºèƒ½è·¯ç”±ä»£ç†å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ {PROXY_PORT}")
    logger.info(f"æœ¬åœ° Ollama åœ°å€: {LOCAL_OLLAMA_BASE_URL}")
    logger.info(f"è™šæ‹Ÿ DeepSeek æ¨¡å‹: {[vm['name'] for vm in VIRTUAL_DEEPSEEK_MODELS]}")
    await fetch_local_models()
    asyncio.create_task(periodic_model_update())

@app.get("/api/tags")
async def get_models():
    """åˆå¹¶æœ¬åœ°æ¨¡å‹å’Œè™šæ‹Ÿæ¨¡å‹ï¼Œè¿”å›ç»™ Copilot"""
    combined_models = local_models_cache.copy()
    
    # æ·»åŠ è™šæ‹Ÿæ¨¡å‹ä¿¡æ¯ï¼Œä¿æŒä¸æœ¬åœ°æ¨¡å‹ç›¸åŒçš„ç»“æ„
    for vm in VIRTUAL_DEEPSEEK_MODELS:
        # æ ¹æ®çœŸå® deepseek-v3.1 æ¨¡å‹çš„æ•°æ®ç»“æ„æ„å»ºè™šæ‹Ÿæ¨¡å‹
        combined_models.append({
            "name": vm["name"],
            "model": vm["name"],  # ä¸ name ç›¸åŒ
            "remote_model": vm.get("actual_model", vm["name"]),  # å®é™…æ¨¡å‹å
            "remote_host": "https://api.deepseek.com",  # DeepSeek API åœ°å€
            "modified_at": "2026-01-14T05:40:00.000000+08:00",  # å½“å‰æ—¶é—´
            "size": 405,  # ä¸çœŸå® cloud æ¨¡å‹ç›¸åŒçš„å¤§å°
            "digest": "d3749919e45f955731da7a7e76849e20f7ed310725d3b8b52822e811f55d0a90",  # ç¤ºä¾‹å“ˆå¸Œ
            "details": {
                "parent_model": "",
                "format": "api",  # ä½¿ç”¨ api æ ¼å¼è¡¨ç¤ºè¿™æ˜¯ API æ¨¡å‹
                "family": "deepseek",
                "families": ["deepseek"],
                "parameter_size": "7B",  # åˆç†çš„å‚æ•°å¤§å°
                "quantization_level": "FP8_E4M3"  # ä¸çœŸå®æ¨¡å‹ç›¸åŒçš„é‡åŒ–çº§åˆ«
            }
        })
    
    result = {"models": combined_models}
    logger.info(f"è¿”å›çš„ /api/tags æ•°æ®: {result}")
    return result

@app.post("/api/generate")
async def generate(request: OllamaGenerateRequest):
    """æ™ºèƒ½è·¯ç”±ç”Ÿæˆè¯·æ±‚"""
    
    # 1. åˆ¤æ–­è¯·æ±‚çš„æ˜¯å¦ä¸ºè™šæ‹Ÿ DeepSeek æ¨¡å‹
    virtual_model = get_virtual_model_info(request.model)
    
    if virtual_model:
        # 2. è·¯ç”±åˆ° DeepSeek API
        logger.info(f"è·¯ç”±åˆ° DeepSeek: {request.model}")
        return await handle_deepseek_request(request, virtual_model["actual_model"])
    else:
        # 3. è·¯ç”±åˆ°æœ¬åœ°çœŸå®çš„ Ollama
        logger.info(f"è·¯ç”±åˆ°æœ¬åœ° Ollama: {request.model}")
        return await handle_local_ollama_request(request)

async def handle_deepseek_request(request: OllamaGenerateRequest, actual_model: str):
    """å¤„ç† DeepSeek API è¯·æ±‚"""
    # è½¬æ¢è¯·æ±‚æ ¼å¼
    messages = [{"role": "user", "content": request.prompt}]
    
    deepseek_data = {
        "model": actual_model,
        "messages": messages,
        "stream": request.stream,
        "temperature": request.options.get("temperature", 0.7),
        "max_tokens": request.options.get("num_predict", 2048),
    }
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # å¤„ç†æµå¼å“åº”
    if request.stream:
        async def deepseek_stream():
            async with httpx.AsyncClient(timeout=30.0) as client:
                async with client.stream("POST", DEEPSEEK_API_URL, json=deepseek_data, headers=headers) as deepseek_response:
                    if deepseek_response.status_code != 200:
                        error_text = await deepseek_response.aread()
                        yield f"data: {json.dumps({'error': error_text.decode()})}\n\n"
                        return
                    
                    async for line in deepseek_response.aiter_lines():
                        if line.startswith("data: "):
                            sse_data = line[6:]
                            if sse_data.strip() == "[DONE]":
                                yield f"data: {json.dumps({'model': request.model, 'done': True})}\n\n"
                                break
                            try:
                                openai_chunk = json.loads(sse_data)
                                if "choices" in openai_chunk and openai_chunk["choices"]:
                                    delta = openai_chunk["choices"][0].get("delta", {})
                                    content = delta.get("content", "")
                                    if content:
                                        ollama_chunk = {
                                            "model": request.model,
                                            "response": content,
                                            "done": False
                                        }
                                        yield f"data: {json.dumps(ollama_chunk)}\n\n"
                            except json.JSONDecodeError:
                                continue
        
        return StreamingResponse(deepseek_stream(), media_type="text/event-stream")
    
    # å¤„ç†éæµå¼å“åº”
    else:
        async with httpx.AsyncClient(timeout=30.0) as client:
            deepseek_response = await client.post(DEEPSEEK_API_URL, json=deepseek_data, headers=headers)
            if deepseek_response.status_code != 200:
                raise HTTPException(status_code=deepseek_response.status_code, 
                                  detail=deepseek_response.text)
            
            openai_result = deepseek_response.json()
            ollama_result = {
                "model": request.model,
                "response": openai_result["choices"][0]["message"]["content"],
                "done": True,
                "total_duration": openai_result.get("usage", {}).get("total_tokens", 0) * 50_000_000,
            }
            return ollama_result

async def handle_local_ollama_request(request: OllamaGenerateRequest):
    """å°†è¯·æ±‚è½¬å‘ç»™æœ¬åœ°çœŸå®çš„ Ollama æœåŠ¡"""
    target_url = f"{LOCAL_OLLAMA_BASE_URL}/api/generate"
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        # å¯¹äºæµå¼è¯·æ±‚ï¼Œç›´æ¥é€ä¼ 
        if request.stream:
            async def local_stream():
                async with client.stream("POST", target_url, 
                                       json=request.model_dump(),
                                       timeout=60.0) as ollama_response:
                    async for chunk in ollama_response.aiter_bytes():
                        yield chunk
            
            return StreamingResponse(local_stream(), 
                                   media_type="application/x-ndjson")
        
        # éæµå¼è¯·æ±‚
        else:
            ollama_response = await client.post(target_url, json=request.model_dump())
            
            if ollama_response.status_code != 200:
                raise HTTPException(status_code=ollama_response.status_code,
                                  detail=ollama_response.text)
            
            return JSONResponse(content=ollama_response.json())

async def fetch_ollama_version():
    """ä»æœ¬åœ° Ollama æœåŠ¡è·å–ç‰ˆæœ¬å·"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            resp = await client.get(f"{LOCAL_OLLAMA_BASE_URL}/api/version")
            if resp.status_code == 200:
                data = resp.json()
                logger.info(f"ä» Ollama è·å–çš„ç‰ˆæœ¬ä¿¡æ¯: {data}")
                return data
    except Exception as e:
        logger.error(f"è·å– Ollama ç‰ˆæœ¬å¤±è´¥: {e}")
    return None

@app.get("/api/version")
async def get_version():
    """è¿”å›ç‰ˆæœ¬ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨çœŸå® Ollama çš„ç‰ˆæœ¬"""
    ollama_version = await fetch_ollama_version()
    if ollama_version:
        return ollama_version
    else:
        # å¦‚æœæ— æ³•è·å–çœŸå®ç‰ˆæœ¬ï¼Œè¿”å›å…¼å®¹ç‰ˆæœ¬
        return {
            "version": "0.6.4"
        }

@app.post("/api/show")
async def show_model(request: Request):
    """
    å¤„ç† /api/show è¯·æ±‚ï¼Œè¿”å›æ¨¡å‹ä¿¡æ¯
    æ ¹æ® Ollama API æ–‡æ¡£ï¼šPOST /api/show éœ€è¦ {"model": "æ¨¡å‹åç§°"}
    """
    try:
        body = await request.json()
        model_name = body.get("model", "")
        logger.info(f"æ”¶åˆ° /api/show è¯·æ±‚ï¼Œæ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè™šæ‹Ÿæ¨¡å‹
        virtual_model = get_virtual_model_info(model_name)
        if virtual_model:
            # è¿”å›è™šæ‹Ÿæ¨¡å‹ä¿¡æ¯ï¼ŒåŸºäºçœŸå® deepseek-v3.1 æ¨¡å‹çš„ç»“æ„
            return {
                "model": model_name,
                "details": {
                    "parent_model": "",
                    "format": "api",  # ä½¿ç”¨ api æ ¼å¼è¡¨ç¤ºè¿™æ˜¯ API æ¨¡å‹
                    "family": "deepseek",
                    "families": ["deepseek"],
                    "parameter_size": "7B",  # åˆç†çš„å‚æ•°å¤§å°
                    "quantization_level": "FP8_E4M3"  # ä¸çœŸå®æ¨¡å‹ç›¸åŒçš„é‡åŒ–çº§åˆ«
                },
                "modelfile": "# Virtual DeepSeek model via API\nFROM api:deepseek\n\n# System prompt\nSYSTEM \"You are a helpful AI assistant.\"",
                "template": "{{ .Prompt }}",  # ä½¿ç”¨ä¸çœŸå®æ¨¡å‹ç›¸åŒçš„æ¨¡æ¿
                "parameters": "num_ctx 4096\nnum_predict 2048\ntemperature 0.7",  # ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸çœŸå®æ¨¡å‹ä¸€è‡´
                "license": "",
                "system": "You are a helpful AI assistant.",
                "remote_model": virtual_model.get("actual_model", model_name),
                "remote_host": "https://api.deepseek.com",
                "model_info": {
                    "general.architecture": "deepseek",
                    "general.basename": virtual_model.get("actual_model", model_name),
                    "deepseek.context_length": 163840,
                    "deepseek.embedding_length": 7168
                },
                "capabilities": ["completion", "tools", "thinking"],  # ä¸ deepseek-v3.1 ç›¸åŒçš„èƒ½åŠ›
                "modified_at": "2026-01-14T05:40:00.000000+08:00"
            }
        else:
            # è½¬å‘åˆ°æœ¬åœ° Ollama
            target_url = f"{LOCAL_OLLAMA_BASE_URL}/api/show"
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(target_url, json=body)
                if response.status_code == 200:
                    response_data = response.json()
                    logger.info(f"æœ¬åœ° Ollama /api/show åŸå§‹è¿”å›: {response_data}")
                    return response_data
                else:
                    # å¦‚æœ Ollama è¿”å›é”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬å“åº”
                    logger.warning(f"Ollama /api/show è¿”å› {response.status_code}, ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
                    return {
                        "model": model_name,
                        "details": {
                            "parent_model": "",
                            "format": "gguf",
                            "family": "unknown",
                            "families": ["unknown"],
                            "parameter_size": "unknown",
                            "quantization_level": "unknown"
                        },
                        "modelfile": "",
                        "template": "",
                        "parameters": {},
                        "license": "",
                        "system": ""
                    }
    except Exception as e:
        logger.error(f"/api/show å¤„ç†é”™è¯¯: {e}")
        return {
            "model": "",
            "details": {},
            "modelfile": "",
            "template": "",
            "parameters": {},
            "license": "",
            "system": ""
        }

@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """
    å¤„ç† OpenAI å…¼å®¹çš„ /v1/chat/completions ç«¯ç‚¹
    æ ¹æ®æ¨¡å‹åç§°è·¯ç”±åˆ°é€‚å½“çš„åç«¯
    """
    try:
        body = await request.json()
        model_name = body.get("model", "")
        messages = body.get("messages", [])
        stream = body.get("stream", False)
        
        logger.info(f"æ”¶åˆ° OpenAI èŠå¤©å®Œæˆè¯·æ±‚ï¼Œæ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}, æµå¼: {stream}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè™šæ‹Ÿ DeepSeek æ¨¡å‹
        virtual_model = get_virtual_model_info(model_name)
        
        if virtual_model:
            # è·¯ç”±åˆ° DeepSeek API
            logger.info(f"è·¯ç”±åˆ° DeepSeek API: {model_name}")
            return await handle_openai_deepseek_request(body, virtual_model["actual_model"])
        else:
            # è·¯ç”±åˆ°æœ¬åœ° Ollama çš„ OpenAI å…¼å®¹ç«¯ç‚¹
            logger.info(f"è·¯ç”±åˆ°æœ¬åœ° Ollama OpenAI ç«¯ç‚¹: {model_name}")
            return await handle_openai_ollama_request(body)
            
    except Exception as e:
        logger.error(f"å¤„ç† /v1/chat/completions é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def handle_openai_deepseek_request(body: dict, actual_model: str):
    """å¤„ç† DeepSeek API çš„ OpenAI æ ¼å¼è¯·æ±‚"""
    # å‡†å¤‡ DeepSeek API è¯·æ±‚
    deepseek_data = {
        "model": actual_model,
        "messages": body.get("messages", []),
        "stream": body.get("stream", False),
        "temperature": body.get("temperature", 0.7),
        "max_tokens": body.get("max_tokens", 2048),
    }
    
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # å¤„ç†æµå¼å“åº”
    if body.get("stream", False):
        async def deepseek_stream():
            async with httpx.AsyncClient(timeout=30.0) as client:
                async with client.stream("POST", DEEPSEEK_API_URL, json=deepseek_data, headers=headers) as deepseek_response:
                    if deepseek_response.status_code != 200:
                        error_text = await deepseek_response.aread()
                        yield f"data: {json.dumps({'error': error_text.decode()})}\n\n"
                        return
                    
                    async for line in deepseek_response.aiter_lines():
                        if line.startswith("data: "):
                            yield line + "\n"
        
        return StreamingResponse(deepseek_stream(), media_type="text/event-stream")
    
    # å¤„ç†éæµå¼å“åº”
    else:
        async with httpx.AsyncClient(timeout=30.0) as client:
            deepseek_response = await client.post(DEEPSEEK_API_URL, json=deepseek_data, headers=headers)
            if deepseek_response.status_code != 200:
                raise HTTPException(status_code=deepseek_response.status_code,
                                  detail=deepseek_response.text)
            
            return JSONResponse(content=deepseek_response.json())

async def handle_openai_ollama_request(body: dict):
    """å°† OpenAI æ ¼å¼è¯·æ±‚è½¬å‘åˆ°æœ¬åœ° Ollama çš„ /v1/chat/completions ç«¯ç‚¹"""
    target_url = f"{LOCAL_OLLAMA_BASE_URL}/v1/chat/completions"
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        # å¯¹äºæµå¼è¯·æ±‚ï¼Œç›´æ¥é€ä¼ 
        if body.get("stream", False):
            async def ollama_stream():
                async with client.stream("POST", target_url,
                                       json=body,
                                       timeout=60.0) as ollama_response:
                    async for chunk in ollama_response.aiter_bytes():
                        yield chunk
            
            return StreamingResponse(ollama_stream(),
                                   media_type="text/event-stream")
        
        # éæµå¼è¯·æ±‚
        else:
            ollama_response = await client.post(target_url, json=body)
            
            if ollama_response.status_code != 200:
                raise HTTPException(status_code=ollama_response.status_code,
                                  detail=ollama_response.text)
            
            return JSONResponse(content=ollama_response.json())

@app.api_route("/api/{path:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def proxy_to_ollama(path: str, request: Request):
    """
    å°†å…¶ä»– Ollama API è¯·æ±‚è½¬å‘åˆ°æœ¬åœ° Ollama æœåŠ¡
    ä¾‹å¦‚ï¼š/api/chat, /api/embeddings, /api/pull, /api/delete ç­‰
    """
    # æ’é™¤å·²ç»ç‰¹æ®Šå¤„ç†çš„ç«¯ç‚¹
    if path in ["tags", "generate", "version", "show"]:
        # è¿™äº›ç«¯ç‚¹å·²ç»æœ‰ç‰¹æ®Šå¤„ç†ï¼Œä¸åº”è¯¥é€šè¿‡æ­¤é€šç”¨è·¯ç”±
        # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œä»ç„¶å…è®¸é€šè¿‡ï¼ˆå®ƒä»¬ä¼šæœ‰è‡ªå·±çš„å¤„ç†ç¨‹åºï¼‰
        pass
    
    target_url = f"{LOCAL_OLLAMA_BASE_URL}/api/{path}"
    logger.info(f"è½¬å‘è¯·æ±‚ {request.method} /api/{path} -> {target_url}")
    
    # è·å–è¯·æ±‚ä½“
    body = None
    if request.method in ["POST", "PUT"]:
        body = await request.body()
    
    # è½¬å‘è¯·æ±‚
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            response = await client.request(
                method=request.method,
                url=target_url,
                content=body,
                headers=dict(request.headers),
                params=dict(request.query_params)
            )
            
            logger.info(f"è½¬å‘æˆåŠŸ {request.method} /api/{path} -> çŠ¶æ€ç : {response.status_code}")
            
            # è¿”å›å“åº”
            return JSONResponse(
                content=response.json() if response.content else None,
                status_code=response.status_code,
                headers=dict(response.headers)
            )
        except Exception as e:
            logger.error(f"è½¬å‘å¤±è´¥ {request.method} /api/{path} -> é”™è¯¯: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Proxy error: {str(e)}")

@app.get("/")
async def root():
    return {
        "message": "Smart Ollama-DeepSeek è·¯ç”±ä»£ç†",
        "endpoints": {
            "GET /api/tags": "è·å–åˆå¹¶åçš„æ¨¡å‹åˆ—è¡¨",
            "POST /api/generate": "æ™ºèƒ½è·¯ç”±ç”Ÿæˆè¯·æ±‚",
            "GET /api/version": "è·å–ç‰ˆæœ¬ä¿¡æ¯",
            "ANY /api/{path}": "è½¬å‘å…¶ä»– Ollama API è¯·æ±‚"
        },
        "config": {
            "proxy_port": PROXY_PORT,
            "local_ollama": LOCAL_OLLAMA_BASE_URL,
            "virtual_models": [vm["name"] for vm in VIRTUAL_DEEPSEEK_MODELS]
        }
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("=" * 60)
    logger.info("ğŸ¤– æ™ºèƒ½ Ollama-DeepSeek è·¯ç”±ä»£ç†")
    logger.info("=" * 60)
    logger.info(f"ğŸ“¡ ä»£ç†æœåŠ¡è¿è¡Œåœ¨: http://localhost:{PROXY_PORT}")
    logger.info(f"ğŸ”— æœ¬åœ° Ollama: {LOCAL_OLLAMA_BASE_URL}")
    logger.info(f"âœ¨ è™šæ‹Ÿæ¨¡å‹: {[vm['name'] for vm in VIRTUAL_DEEPSEEK_MODELS]}")
    logger.info("")
    logger.info("ğŸ’¡ è¯·åœ¨ Copilot ä¸­é…ç½® Ollama åœ°å€ä¸ºä¸Šè¿°ä»£ç†åœ°å€")
    logger.info("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=PROXY_PORT)