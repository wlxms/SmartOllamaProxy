#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°å¢çš„æ€§èƒ½ç›‘æ§ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import httpx
import json
import sys
from typing import Dict, Any


class PerformanceMonitorTest:
    """æ€§èƒ½ç›‘æ§æµ‹è¯•ç±»"""

    def __init__(self, base_url: str = "http://localhost:11435"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=60.0)

    async def test_streaming_request(self):
        """æµ‹è¯•æµå¼è¯·æ±‚çš„æ€§èƒ½ç›‘æ§"""
        print("=" * 80)
        print("æµ‹è¯•æµå¼è¯·æ±‚çš„æ€§èƒ½ç›‘æ§")
        print("=" * 80)

        request_data = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»Python"}
            ],
            "stream": True
        }

        print(f"\nå‘é€æµå¼è¯·æ±‚åˆ° {self.base_url}/v1/chat/completions")
        print(f"è¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}\n")

        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/v1/chat/completions",
                json=request_data,
                headers={"Content-Type": "application/json"}
            ) as response:
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")

                content_lines = []
                async for line in response.aiter_lines():
                    if line.strip():
                        if line.startswith("data: "):
                            data_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
                            if data_str == "[DONE]":
                                break
                            try:
                                data = json.loads(data_str)
                                if "choices" in data and len(data["choices"]) > 0:
                                    delta = data["choices"][0].get("delta", {})
                                    content = delta.get("content", "")
                                    if content:
                                        content_lines.append(content)
                                        print(content, end="", flush=True)
                            except json.JSONDecodeError:
                                pass

                print("\n\næµå¼å“åº”æ¥æ”¶å®Œæˆ")

                # æ£€æŸ¥æ˜¯å¦æ¥æ”¶åˆ°å†…å®¹
                if content_lines:
                    print(f"âœ… æ¥æ”¶åˆ° {len(content_lines)} ä¸ªå†…å®¹å—")
                else:
                    print("âš ï¸  æœªæ¥æ”¶åˆ°å†…å®¹å—")

                print("\né¢„æœŸçœ‹åˆ°çš„æ€§èƒ½ç›‘æ§æ—¥å¿—:")
                print("- [OPENAI /v1/chat/completions] è½¬å‘å‰è€—æ—¶: X.XXXç§’")
                print("- [OPENAI /v1/chat/completions] å¼€å§‹è½¬å‘åˆ°åç«¯")
                print("- [OpenAIBackendRouter] é¦–å—å“åº”æ—¶é—´: X.XXXç§’")
                print("- [OpenAIBackendRouter] é¦–å—åˆ°å…¨éƒ¨å—æ¥æ”¶è€—æ—¶: X.XXXç§’")
                print("- [OPENAI /v1/chat/completions] åç«¯è½¬å‘è€—æ—¶: X.XXXç§’")
                print("- [OPENAI /v1/chat/completions] æ€»è€—æ—¶: X.XXXç§’")

        except httpx.ConnectError as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ä»£ç†æœåŠ¡æ­£åœ¨è¿è¡Œ: python main.py")
            return False
        except httpx.TimeoutException as e:
            print(f"âŒ è¯·æ±‚è¶…æ—¶: {e}")
            return False
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return False

        return True

    async def test_non_streaming_request(self):
        """æµ‹è¯•éæµå¼è¯·æ±‚çš„æ€§èƒ½ç›‘æ§"""
        print("\n" + "=" * 80)
        print("æµ‹è¯•éæµå¼è¯·æ±‚çš„æ€§èƒ½ç›‘æ§")
        print("=" * 80)

        request_data = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "user", "content": "1+1=?"}
            ],
            "stream": False
        }

        print(f"\nå‘é€éæµå¼è¯·æ±‚åˆ° {self.base_url}/v1/chat/completions")
        print(f"è¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}\n")

        try:
            response = await self.client.post(
                f"{self.base_url}/v1/chat/completions",
                json=request_data,
                headers={"Content-Type": "application/json"}
            )

            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                data = response.json()
                print("âœ… è¯·æ±‚æˆåŠŸ")

                if "choices" in data and len(data["choices"]) > 0:
                    content = data["choices"][0]["message"]["content"]
                    print(f"å“åº”å†…å®¹: {content}")

                print("\né¢„æœŸçœ‹åˆ°çš„æ€§èƒ½ç›‘æ§æ—¥å¿—:")
                print("- [OPENAI /v1/chat/completions] è½¬å‘å‰è€—æ—¶: X.XXXç§’")
                print("- [OPENAI /v1/chat/completions] å¼€å§‹è½¬å‘åˆ°åç«¯")
                print("- [OPENAI /v1/chat/completions] åç«¯è½¬å‘è€—æ—¶: X.XXXç§’")
                print("- [OPENAI /v1/chat/completions] æ€»è€—æ—¶: X.XXXç§’")
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.text}")
                return False

        except httpx.ConnectError as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ä»£ç†æœåŠ¡æ­£åœ¨è¿è¡Œ: python main.py")
            return False
        except httpx.TimeoutException as e:
            print(f"âŒ è¯·æ±‚è¶…æ—¶: {e}")
            return False
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return False

        return True

    async def test_generate_endpoint(self):
        """æµ‹è¯•Ollama /api/generateç«¯ç‚¹çš„æ€§èƒ½ç›‘æ§"""
        print("\n" + "=" * 80)
        print("æµ‹è¯• /api/generate ç«¯ç‚¹çš„æ€§èƒ½ç›‘æ§")
        print("=" * 80)

        request_data = {
            "model": "llama3.2:latest",
            "prompt": "Hello",
            "stream": True
        }

        print(f"\nå‘é€æµå¼è¯·æ±‚åˆ° {self.base_url}/api/generate")
        print(f"è¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}\n")

        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/api/generate",
                json=request_data,
                headers={"Content-Type": "application/json"}
            ) as response:
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")

                chunk_count = 0
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if "response" in data:
                                chunk_count += 1
                                print(data["response"], end="", flush=True)
                            if data.get("done", False):
                                break
                        except json.JSONDecodeError:
                            pass

                print("\n\næµå¼å“åº”æ¥æ”¶å®Œæˆ")

                if chunk_count > 0:
                    print(f"âœ… æ¥æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
                else:
                    print("âš ï¸  æœªæ¥æ”¶åˆ°æ•°æ®å—")

                print("\né¢„æœŸçœ‹åˆ°çš„æ€§èƒ½ç›‘æ§æ—¥å¿—:")
                print("- [OLLAMA /api/generate] æ”¶åˆ°æµå¼è¯·æ±‚ï¼Œæ¨¡å‹: llama3.2:latest")
                print("- [OllamaBackendRouter] è¿æ¥å»ºç«‹è€—æ—¶: X.XXXç§’")
                print("- [OllamaBackendRouter] é¦–å—å“åº”æ—¶é—´: X.XXXç§’")
                print("- [OllamaBackendRouter] é¦–å—åˆ°å…¨éƒ¨å—æ¥æ”¶è€—æ—¶: X.XXXç§’")
                print("- [OllamaBackendRouter] æµå¼è¯·æ±‚å®Œæˆï¼Œæ€»è€—æ—¶: X.XXXç§’")
                print("- [OLLAMA /api/generate] æ€»è€—æ—¶: X.XXXç§’")

        except httpx.ConnectError as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ä»£ç†æœåŠ¡æ­£åœ¨è¿è¡Œ: python main.py")
            return False
        except httpx.TimeoutException as e:
            print(f"âŒ è¯·æ±‚è¶…æ—¶: {e}")
            return False
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return False

        return True

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€" * 40)
    print("Smart Ollama Proxy - æ€§èƒ½ç›‘æ§æµ‹è¯•")
    print("ğŸš€" * 40 + "\n")

    tester = PerformanceMonitorTest()

    try:
        # æµ‹è¯•1: æµå¼è¯·æ±‚
        result1 = await tester.test_streaming_request()
        if not result1:
            print("\nâš ï¸  æµå¼è¯·æ±‚æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡å…¶ä»–æµ‹è¯•")
            return

        # ç­‰å¾…ä¸€ä¸‹
        await asyncio.sleep(1)

        # æµ‹è¯•2: éæµå¼è¯·æ±‚
        result2 = await tester.test_non_streaming_request()
        if not result2:
            print("\nâš ï¸  éæµå¼è¯·æ±‚æµ‹è¯•å¤±è´¥")

        # ç­‰å¾…ä¸€ä¸‹
        await asyncio.sleep(1)

        # æµ‹è¯•3: Ollama generateç«¯ç‚¹
        result3 = await tester.test_generate_endpoint()
        if not result3:
            print("\nâš ï¸  /api/generate æµ‹è¯•å¤±è´¥")

        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        print("\nè¯·æŸ¥çœ‹ä»£ç†æœåŠ¡æ—¥å¿—ï¼Œç¡®è®¤æ‰€æœ‰æ€§èƒ½ç›‘æ§æŒ‡æ ‡æ˜¯å¦æ­£ç¡®è¾“å‡ºã€‚")
        print("\næ—¥å¿—æ–‡ä»¶ä½ç½®: logs/ ç›®å½•ä¸‹")

    finally:
        await tester.close()


if __name__ == "__main__":
    asyncio.run(main())
