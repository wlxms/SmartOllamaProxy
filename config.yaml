# Smart Ollama Proxy 配置
# 模型配置格式：每个模型包含支持的后端模式

# 代理服务配置
proxy:
  port: 11435
  host: "0.0.0.0"
  log_level: "INFO"
  # 是否启用详细的JSON日志记录（会打印完整的请求/响应JSON数据）
  verbose_json_logging: true
  # 是否启用工具压缩优化（检测重复工具列表并压缩）
  tool_compression_enabled: true
  # 是否启用重复提示词压缩优化（从内容头开始比对与上次内容，将重复部分替换为标记）
  prompt_compression_enabled: true
  # 是否启用HTTP传输压缩（gzip/deflate）
  http_compression_enabled: true


# 日志配置（智能统一日志系统）
logging:
  enabled: true
  log_dir: "logs"
  log_level: "INFO"
  # 日志类型配置
  log_types:
    process:
      enabled: true
      save_to_file: true
      show_in_console: true
      async_mode: true
    performance:
      enabled: true
      save_to_file: true
      show_in_console: false
      async_mode: false  # 性能日志需要即时性
    data:
      enabled: true
      save_to_file: false  # 数据日志通常不保存到文件，避免磁盘占用
      show_in_console: false  # 在控制台显示数据摘要
      async_mode: true
    progress:
      enabled: true
      save_to_file: false  # 进度条永不保存到文件
      show_in_console: true  # 进度条总是在控制台显示
      async_mode: false  # 进度显示需要即时
  # 性能配置
  performance:
    max_queue_size: 1000
    max_workers: 4
    flush_interval: 1.0
  # 进度条显示配置
  progress:
    width: 50
    fill_char: "█"
    empty_char: "░"
    show_percentage: true
    show_elapsed_time: true
  # 文件轮转配置
  file_rotation:
    max_size_mb: 100
    backup_count: 5

# 本地 Ollama 配置（用于本地模型）
local_ollama:
  base_url: "http://localhost:11434"
  timeout: 60

# 模型配置
models:
  # DeepSeek 模型组
  deepseek:
    description: "DeepSeek V3.2 系列模型"
    available_models:
      deepseek-chat:
        context_length: 128000
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "deepseek-chat"
      deepseek-reasoner:
        context_length: 128000
        embedding_length: 6400
        capabilities: ["completion", "tools", "thinking"]
        actual_model: "deepseek-reasoner"
    
    # 后端配置（按配置顺序决定优先级，如果前一个失败则尝试后一个）
    # LiteLLM兼容后端配置（优先级1）
    litellm_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-***"
      timeout: 30
      max_retries: 3
      cache: true
      # compression_enabled: true  # 是否启用HTTP压缩（默认true，继承全局proxy.http_compression_enabled）

    # OpenAI兼容后端配置（优先级2）
    openai_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-***"
      timeout: 30

  # 硅基流动
  siliconflow:
    description: "硅基流动系列模型"
    available_models:
      siliconflow-deepseek-chat:
        context_length: 160000
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "deepseek-ai/DeepSeek-V3.2"
    
    # OpenAI兼容后端配置
    openai_backend:
      base_url: "https://api.siliconflow.cn/v1"
      api_key: "sk-***"
      timeout: 30
      

  # # OpenAI 模型组
  # openai:
  #   description: "OpenAI GPT 系列模型"
  #   available_models:
  #     gpt-4o:
  #       context_length: 128000
  #       embedding_length: 6400
  #       capabilities: ["completion", "tools"]
  #       actual_model: "gpt-4o"
  #     gpt-4o-mini:
  #       context_length: 128000
  #       embedding_length: 6400
  #       capabilities: ["completion", "tools"]
  #       actual_model: "gpt-4o-mini"
  #     gpt-4-turbo:
  #       context_length: 128000
  #       embedding_length: 6400
  #       capabilities: ["completion", "tools"]
  #       actual_model: "gpt-4-turbo"
  #     gpt-3.5-turbo:
  #       context_length: 16384
  #       embedding_length: 1536
  #       capabilities: ["completion"]
  #       actual_model: "gpt-3.5-turbo"
    
  #   openai_backend:
  #     base_url: "https://api.openai.com/v1"
  #     api_key: "sk-***"
  #     timeout: 30

  # # Anthropic Claude 模型组
  # claude:
  #   description: "Anthropic Claude 系列模型"
  #   available_models:
  #     claude-3-5-sonnet:
  #       context_length: 200000
  #       embedding_length: 8192
  #       capabilities: ["completion", "tools"]
  #       actual_model: "claude-3-5-sonnet-20241022"
  #     claude-3-opus:
  #       context_length: 200000
  #       embedding_length: 8192
  #       capabilities: ["completion", "tools"]
  #       actual_model: "claude-3-opus-20240229"
  #     claude-3-haiku:
  #       context_length: 200000
  #       embedding_length: 8192
  #       capabilities: ["completion", "tools"]
  #       actual_model: "claude-3-haiku-20240307"
    
  #   openai_backend:
  #     base_url: "https://api.anthropic.com/v1"
  #     api_key: "sk-***"
  #     timeout: 30
  #     # Claude需要特殊的请求头
  #     headers:
  #       "anthropic-version": "2023-06-01"
  #       "content-type": "application/json"

  # # Groq 模型组
  # groq:
  #   description: "Groq 高速推理模型"
  #   available_models:
  #     llama-3.3-70b:
  #       context_length: 32768
  #       embedding_length: 4096
  #       capabilities: ["completion"]
  #       actual_model: "llama-3.3-70b-versatile"
  #     mixtral-8x7b:
  #       context_length: 32768
  #       embedding_length: 4096
  #       capabilities: ["completion"]
  #       actual_model: "mixtral-8x7b-32768"
  #     gemma-7b:
  #       context_length: 8192
  #       embedding_length: 2560
  #       capabilities: ["completion"]
  #       actual_model: "gemma-7b-it"
    
  #   openai_backend:
  #     base_url: "https://api.groq.com/openai/v1"
  #     api_key: "sk-***"
  #     timeout: 30

  # 通义千问模型组
  qwen:
    description: "通义千问系列模型"
    available_models:
      qwen3-max:
        context_length: 262144
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "qwen3-max"
      # qwen3-coder-plus:
      #   context_length: 1000000
      #   embedding_length: 6400
      #   capabilities: ["completion", "tools"]
      #   actual_model: "qwen3-coder-plus"
      qwen3-coder-flash:
        context_length: 1000000
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "qwen3-coder-flash"
      ali-deepseek-chat:
        context_length: 128000
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "deepseek-v3.2"
      ali-deepseek-reasoner:
        context_length: 128000
        embedding_length: 6400
        capabilities: ["completion", "tools", "thinking"]
        actual_model: "deepseek-v3.2"
    
    # OpenAI兼容后端配置（通义千问使用DashScope API）
    openai_backend:
      base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
      api_key: "sk-***"
      timeout: 30

  qwen-coder:
    description: "通义千问Coder订阅"
    available_models:
      qwen3-coder-plus:
        context_length: 1000000
        embedding_length: 6400
        capabilities: ["completion", "tools"]
        actual_model: "qwen3-coder-plus"
    
    # OpenAI兼容后端配置（通义千问使用DashScope API）
    openai_backend:
      base_url: "https://coding.dashscope.aliyuncs.com/v1"
      api_key: "sk-***"
      timeout: 30
      use_litellm: false  # 默认启用，如果安装了litellm包
      litellm_params:
        max_retries: 3
        cache: true
        timeout: 30
      # compression_enabled: true  # 是否启用HTTP压缩（默认true，继承全局proxy.http_compression_enabled）
      # compression_enabled: true  # 是否启用HTTP压缩（默认true，继承全局proxy.http_compression_enabled）

  # 本地模型组（从本地Ollama获取）
  local:
    description: "本地 Ollama 模型"
    available_models: {}  # 自动从本地Ollama获取
    # 本地模型使用 local_ollama 配置，不需要后端配置

# 路由配置
routing:
  # 默认后端模式（当请求没有指定时使用）
  default_backend_mode: "openai_backend"
  
  # 是否启用自动模型发现
  auto_discover_local_models: true
  
  # 模型缓存配置
  cache:
    enabled: true
    update_interval: 60  # 秒