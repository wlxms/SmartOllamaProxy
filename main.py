# smart_ollama_proxy.py - é‡æ„ç‰ˆæœ¬
# æ”¯æŒå¤šæ¨¡å‹åç«¯é…ç½®ï¼ŒåŸºäºOpenAIå…¼å®¹æ¨¡å¼ï¼Œä½¿ç”¨backendè·¯ç”±å™¨æé«˜æ‰©å±•æ€§

import sys
import io
import logging
from utils import json
import asyncio
from typing import Optional, List, Dict, Any, Tuple

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import httpx

from config_loader import ConfigLoader, ModelRouter, BackendConfig
from routers.backend_router_factory import BackendRouterFactory, BackendManager

# ============ åˆå§‹åŒ– ============

# é…ç½® logging
import os
from datetime import datetime
from stream_logger import init_global_logger, configure_root_logging, get_global_logger

# åˆ›å»ºlogsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
log_dir = "logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# åˆå§‹åŒ–å…¨å±€æ—¥å¿—è®°å½•å™¨
global_logger = init_global_logger(
    log_dir=log_dir,
    max_workers=4,
    max_queue_size=1000,
    enabled=True,
    verbose_json_logging=False,
    log_level="DEBUG",
    enable_file_logging=True,
    enable_console_logging=True
)

# é…ç½®æ ‡å‡†loggingæ¨¡å—ï¼Œå°†æ‰€æœ‰æ—¥å¿—é‡å®šå‘åˆ°GlobalLogger
configure_root_logging(
    level=logging.INFO,
    global_logger=global_logger
)

# ä¿æŒåŸºæœ¬çš„æ§åˆ¶å°æ—¥å¿—é…ç½®ï¼ˆç”¨äºæ—©æœŸæ—¥å¿—è®°å½•ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("smart_ollama_proxy")
logger.info(f"å…¨å±€æ—¥å¿—è®°å½•å™¨å·²åˆå§‹åŒ–ï¼Œæ—¥å¿—ç›®å½•: {log_dir}")

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆé¿å…æ–‡ä»¶å¥æŸ„å…³é—­é—®é¢˜ï¼‰
if sys.stdout.encoding.lower() != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# åˆå§‹åŒ–é…ç½®å’Œè·¯ç”±
config_loader = ConfigLoader("config.yaml")
model_router = ModelRouter(config_loader)

# åˆå§‹åŒ–åç«¯ç®¡ç†å™¨
backend_manager = BackendManager()

# å…¨å±€å˜é‡ï¼šæ˜¯å¦æ¨¡æ‹Ÿollamaè¿æ¥è¶…æ—¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰
SIMULATE_OLLAMA_TIMEOUT = False

# æ˜¯å¦å¯ç”¨è¯¦ç»†çš„JSONæ—¥å¿—è®°å½•
VERBOSE_JSON_LOGGING = config_loader.get_verbose_json_logging()
logger.info(f"è¯¦ç»†çš„JSONæ—¥å¿—è®°å½•: {'å¯ç”¨' if VERBOSE_JSON_LOGGING else 'ç¦ç”¨'}")

# åç«¯é…ç½®æ˜ å°„è¡¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šé¿å…æ¯æ¬¡è¯·æ±‚éƒ½éå†ï¼‰
# é”®: (base_url, api_key, backend_mode) çš„å…ƒç»„ï¼Œå€¼: router_name
_backend_config_map: Dict[tuple, str] = {}

# Ollama å¯ç”¨æ€§æ£€æŸ¥ç¼“å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
_ollama_available_cache = {"result": None, "timestamp": 0, "ttl": 5}

# ============ æ•°æ®æ¨¡å‹ ============

class OllamaGenerateRequest(BaseModel):
    model: str
    prompt: str
    stream: bool = False
    options: dict = {}


# ============ è¾…åŠ©å‡½æ•° ============

def init_backend_routers():
    """åˆå§‹åŒ–åç«¯è·¯ç”±å™¨"""
    global _backend_config_map
    _backend_config_map.clear()
    
    # è·å–å‹ç¼©ä¼˜åŒ–é…ç½®
    tool_compression_enabled = config_loader.get_tool_compression_enabled()
    prompt_compression_enabled = config_loader.get_prompt_compression_enabled()
    
    # è·å–æ‰€æœ‰åç«¯é…ç½®
    backend_configs = config_loader.get_all_backend_configs()
    
    for backend_name, backend_config in backend_configs.items():
        try:
            # è·å–å‹ç¼©ä¼˜åŒ–é…ç½®
            tool_compression_enabled = config_loader.get_tool_compression_enabled()
            prompt_compression_enabled = config_loader.get_prompt_compression_enabled()
            # åˆ›å»ºè·¯ç”±å™¨
            router = BackendRouterFactory.create_router(
                backend_config,
                verbose_json_logging=VERBOSE_JSON_LOGGING,
                tool_compression_enabled=tool_compression_enabled,
                prompt_compression_enabled=prompt_compression_enabled
            )
            # æ³¨å†Œåˆ°ç®¡ç†å™¨
            backend_manager.register_router(backend_name, router)
            
            # æ„å»ºæ˜ å°„è¡¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            backend_mode = backend_config.backend_mode or ""
            config_key = (backend_config.base_url, backend_config.api_key, backend_mode)
            _backend_config_map[config_key] = backend_name
            
            logger.info(f"åˆå§‹åŒ–åç«¯è·¯ç”±å™¨: {backend_name}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åç«¯è·¯ç”±å™¨ {backend_name} å¤±è´¥: {e}")
    
    # åˆå§‹åŒ–æœ¬åœ°è·¯ç”±å™¨ï¼ˆæ€»æ˜¯æ³¨å†Œmockè·¯ç”±å™¨ï¼ŒçœŸå®è·¯ç”±å™¨åœ¨éœ€è¦æ—¶åˆ›å»ºï¼‰
    local_config = config_loader.get_local_ollama_config()
    base_url = local_config.get("base_url", "http://localhost:11434")
    
    # æ€»æ˜¯æ³¨å†Œmockè·¯ç”±å™¨
    mock_backend_config = BackendConfig({
        "base_url": "http://mock.local",
        "timeout": 60
    })
    mock_router = BackendRouterFactory.create_router(
        mock_backend_config,
        "mock",
        verbose_json_logging=VERBOSE_JSON_LOGGING,
        tool_compression_enabled=tool_compression_enabled,
        prompt_compression_enabled=prompt_compression_enabled
    )
    backend_manager.register_router("mock", mock_router)
    logger.info("åˆå§‹åŒ–æ¨¡æ‹Ÿè·¯ç”±å™¨ï¼ˆå¤‡ç”¨ï¼‰")
    
    # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨ï¼ˆä»…åœ¨å¯åŠ¨æ—¶æ£€æŸ¥ï¼Œä½†æ¯æ¬¡APIè°ƒç”¨æ—¶ä¼šé‡æ–°æ£€æŸ¥ï¼‰
    ollama_available = False
    if not SIMULATE_OLLAMA_TIMEOUT:
        try:
            import socket
            # å°è¯•è¿æ¥ç«¯å£
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2.0)  # 2ç§’è¶…æ—¶
            result = sock.connect_ex(('localhost', 11434))
            sock.close()
            ollama_available = (result == 0)
        except:
            ollama_available = False
    
    if ollama_available and not SIMULATE_OLLAMA_TIMEOUT:
        # ä½¿ç”¨çœŸå®çš„Ollamaè·¯ç”±å™¨
        local_backend_config = BackendConfig({
            "base_url": base_url,
            "timeout": local_config.get("timeout", 60)
        })
        local_router = BackendRouterFactory.create_router(
            local_backend_config,
            "ollama",
            verbose_json_logging=VERBOSE_JSON_LOGGING,
            tool_compression_enabled=tool_compression_enabled,
            prompt_compression_enabled=prompt_compression_enabled
        )
        backend_manager.register_router("local", local_router)
        logger.info("åˆå§‹åŒ–æœ¬åœ°Ollamaè·¯ç”±å™¨")
    else:
        # ä½¿ç”¨æ¨¡æ‹Ÿè·¯ç”±å™¨ä½œä¸ºæœ¬åœ°è·¯ç”±å™¨
        backend_manager.register_router("local", mock_router)
        if SIMULATE_OLLAMA_TIMEOUT:
            logger.info("æ¨¡æ‹ŸOllamaè¿æ¥è¶…æ—¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè·¯ç”±å™¨ä½œä¸ºæœ¬åœ°è·¯ç”±å™¨")
        else:
            logger.info("Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè·¯ç”±å™¨ä½œä¸ºæœ¬åœ°è·¯ç”±å™¨")


async def check_ollama_available() -> bool:
    """æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    import time
    
    if SIMULATE_OLLAMA_TIMEOUT:
        logger.info("æ¨¡æ‹ŸOllamaè¿æ¥è¶…æ—¶ï¼Œè¿”å›ä¸å¯ç”¨")
        return False
    
    # æ£€æŸ¥ç¼“å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    current_time = time.time()
    if (_ollama_available_cache["result"] is not None and 
        current_time - _ollama_available_cache["timestamp"] < _ollama_available_cache["ttl"]):
        return _ollama_available_cache["result"]
    
    local_config = config_loader.get_local_ollama_config()
    base_url = local_config.get("base_url", "http://localhost:11434")
    
    try:
        # ä½¿ç”¨HTTPè¯·æ±‚æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨ï¼ˆä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´ï¼‰
        async with httpx.AsyncClient(timeout=1.0) as client:
            resp = await client.get(f"{base_url}/api/tags")
            result = resp.status_code == 200
    except (httpx.ConnectError, httpx.TimeoutException, httpx.ReadTimeout, httpx.ConnectTimeout) as e:
        logger.debug(f"Ollamaè¿æ¥æ£€æŸ¥å¤±è´¥: {type(e).__name__}")
        result = False
    except Exception as e:
        logger.debug(f"Ollamaè¿æ¥æ£€æŸ¥å¼‚å¸¸: {type(e).__name__}")
        result = False
    
    # æ›´æ–°ç¼“å­˜
    _ollama_available_cache["result"] = result
    _ollama_available_cache["timestamp"] = current_time
    
    return result


async def get_backend_candidates_for_model(model_name: str) -> List[Tuple[str, Optional[BackendConfig], str]]:
    """
    è·å–æ¨¡å‹å¯¹åº”çš„åç«¯è·¯ç”±å™¨å€™é€‰åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    
    Returns:
        [(router_name, backend_config, actual_model), ...] 
        å¦‚æœä½¿ç”¨æœ¬åœ°Ollamaï¼Œè¿”å›[("local", None, model_name)]
    """
    # è·¯ç”±è¯·æ±‚
    backend_infos = await model_router.route_request(model_name)
    
    logger.info(f"æ¨¡å‹ {model_name} çš„è·¯ç”±ç»“æœ: {len(backend_infos) if backend_infos else 0} ä¸ªåç«¯é…ç½®")
    
    if backend_infos is None:
        # ä½¿ç”¨æœ¬åœ°Ollama
        logger.info(f"æ¨¡å‹ {model_name} ä½¿ç”¨æœ¬åœ°Ollama")
        return [("local", None, model_name)]
    
    candidates = []
    for backend_config, actual_model in backend_infos:
        # ä½¿ç”¨æ˜ å°„è¡¨æŸ¥æ‰¾è·¯ç”±å™¨åç§°ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šO(1)æŸ¥æ‰¾æ›¿ä»£O(n)éå†ï¼‰
        backend_mode = backend_config.backend_mode or ""
        config_key = (backend_config.base_url, backend_config.api_key, backend_mode)
        router_name = _backend_config_map.get(config_key)
        
        if router_name:
            # æ£€æŸ¥è·¯ç”±å™¨æ˜¯å¦å·²æ³¨å†Œï¼ˆé˜²æ­¢è¢«æ„å¤–åˆ é™¤ï¼‰
            router = backend_manager.get_router(router_name)
            if router:
                logger.debug(f"å¤ç”¨å·²å­˜åœ¨çš„è·¯ç”±å™¨: {router_name} (base_url: {backend_config.base_url})")
                candidates.append((router_name, backend_config, actual_model))
                continue
            else:
                logger.warning(f"æ˜ å°„è¡¨ä¸­çš„è·¯ç”±å™¨ {router_name} ä¸å­˜åœ¨ï¼Œé‡æ–°åˆ›å»º")
                # ä»æ˜ å°„è¡¨ä¸­ç§»é™¤æ— æ•ˆæ¡ç›®
                _backend_config_map.pop(config_key, None)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒé…ç½®çš„è·¯ç”±å™¨ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        # éå†å·²æ³¨å†Œçš„è·¯ç”±å™¨ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„é…ç½®
        found = False
        for existing_name, existing_router in backend_manager.routers.items():
            if hasattr(existing_router, 'config'):
                existing_config = existing_router.config
                if (existing_config.base_url == backend_config.base_url and 
                    existing_config.api_key == backend_config.api_key and
                    getattr(existing_config, 'backend_mode', None) == backend_config.backend_mode):
                    # æ‰¾åˆ°åŒ¹é…çš„è·¯ç”±å™¨ï¼Œæ›´æ–°æ˜ å°„è¡¨å¹¶å¤ç”¨
                    logger.debug(f"æ‰¾åˆ°åŒ¹é…çš„è·¯ç”±å™¨: {existing_name}ï¼Œå¤ç”¨è€Œä¸æ˜¯åˆ›å»ºæ–°çš„")
                    _backend_config_map[config_key] = existing_name
                    candidates.append((existing_name, backend_config, actual_model))
                    found = True
                    break
        
        if found:
            continue
        
        # å¦‚æœç¡®å®æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºä¸€ä¸ªåŸºäºåç«¯é…ç½®çš„è·¯ç”±å™¨
        # ä½¿ç”¨ç®€åŒ–åç§°ï¼šåŸºäºbase_urlçš„åŸŸå
        from urllib.parse import urlparse
        
        try:
            parsed_url = urlparse(backend_config.base_url)
            domain = parsed_url.netloc.replace('.', '_')
            router_name = f"backend_{domain}"
            
            # æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ åç¼€
            if router_name in backend_manager.routers:
                import hashlib
                # ä½¿ç”¨é…ç½®çš„å“ˆå¸Œå€¼ä½œä¸ºåç¼€ï¼Œç¡®ä¿å”¯ä¸€æ€§
                config_hash = hashlib.md5(f"{backend_config.base_url}{backend_config.api_key}".encode()).hexdigest()[:8]
                router_name = f"{router_name}_{config_hash}"
            
            # åˆ›å»ºå¹¶æ³¨å†Œè·¯ç”±å™¨
            logger.info(f"åˆ›å»ºæ–°çš„è·¯ç”±å™¨: {router_name} (base_url: {backend_config.base_url})")
            router = BackendRouterFactory.create_router(backend_config, verbose_json_logging=VERBOSE_JSON_LOGGING)
            backend_manager.register_router(router_name, router)
            
            # æ›´æ–°æ˜ å°„è¡¨
            _backend_config_map[config_key] = router_name
            
            candidates.append((router_name, backend_config, actual_model))
        except Exception as e:
            logger.error(f"åˆ›å»ºè·¯ç”±å™¨å¤±è´¥: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åç§°
            router_name = "openai_compatible"
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if router_name in backend_manager.routers:
                existing_router = backend_manager.routers[router_name]
                if (hasattr(existing_router, 'config') and
                    existing_router.config.base_url == backend_config.base_url and
                    existing_router.config.api_key == backend_config.api_key and
                    getattr(existing_router.config, 'backend_mode', None) == backend_config.backend_mode):
                    _backend_config_map[config_key] = router_name
                    candidates.append((router_name, backend_config, actual_model))
                    continue
            
            router = BackendRouterFactory.create_router(backend_config, verbose_json_logging=VERBOSE_JSON_LOGGING)
            backend_manager.register_router(router_name, router)
            
            # æ›´æ–°æ˜ å°„è¡¨
            _backend_config_map[config_key] = router_name
            
            candidates.append((router_name, backend_config, actual_model))
    
    logger.info(f"æ¨¡å‹ {model_name} çš„å€™é€‰è·¯ç”±å™¨: {[c[0] for c in candidates]}")
    return candidates


async def get_backend_router_for_model(model_name: str) -> Optional[Tuple[str, Optional[BackendConfig], str]]:
    """
    è·å–æ¨¡å‹å¯¹åº”çš„åç«¯è·¯ç”±å™¨ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€™é€‰ï¼‰
    
    Returns:
        (router_name, backend_config, actual_model) æˆ– None
    """
    candidates = await get_backend_candidates_for_model(model_name)
    if not candidates:
        return None
    return candidates[0]


async def try_backend_request(
    model_name: str,
    request_data: Dict[str, Any],
    stream: bool,
    convert_to_ollama: bool = False,
    support_thinking: bool = False,
    endpoint: str = "generate"
) -> Any:
    """
    å°è¯•ä½¿ç”¨å€™é€‰åç«¯åˆ—è¡¨å¤„ç†è¯·æ±‚ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨å›é€€
    
    Args:
        model_name: æ¨¡å‹åç§°
        request_data: è¯·æ±‚æ•°æ®
        stream: æ˜¯å¦æµå¼
        convert_to_ollama: æ˜¯å¦è½¬æ¢ä¸ºOllamaæ ¼å¼
        support_thinking: æ˜¯å¦æ”¯æŒthinkingèƒ½åŠ›
        endpoint: ç«¯ç‚¹ç±»å‹ ('generate' æˆ– 'chat')
    
    Returns:
        å“åº”æ•°æ®
        
    Raises:
        å¦‚æœæ‰€æœ‰åç«¯éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
    """
    candidates = await get_backend_candidates_for_model(model_name)
    if not candidates:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ¨¡å‹: {model_name}")
    
    last_exception = None
    for i, (router_name, backend_config, actual_model) in enumerate(candidates):
        try:
            logger.info(f"å°è¯•åç«¯ {i+1}/{len(candidates)}: {router_name}")
            
            if router_name == "local":
                # æœ¬åœ°Ollamaå¤„ç†
                local_config = config_loader.get_local_ollama_config()
                base_url = local_config.get("base_url", "http://localhost:11434")
                # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
                ollama_available = await check_ollama_available()
                if not ollama_available:
                    logger.info(f"Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè·¯ç”±å™¨å¤„ç†æœ¬åœ°æ¨¡å‹è¯·æ±‚: {model_name}")
                    router_name = "mock"
            
            # é€šè¿‡åç«¯è·¯ç”±å™¨å¤„ç†
            response = await backend_manager.handle_request(
                router_name,
                actual_model,
                request_data,
                stream,
                convert_to_ollama=convert_to_ollama,
                virtual_model=model_name,
                support_thinking=support_thinking
            )
            logger.info(f"åç«¯ {router_name} è¯·æ±‚æˆåŠŸ")
            return response
        except Exception as e:
            logger.warning(f"åç«¯ {router_name} è¯·æ±‚å¤±è´¥: {type(e).__name__}: {e}")
            last_exception = e
            continue
    
    # æ‰€æœ‰åç«¯éƒ½å¤±è´¥
    logger.error(f"æ‰€æœ‰åç«¯éƒ½å¤±è´¥ï¼Œæœ€åä¸€ä¸ªé”™è¯¯: {last_exception}")
    if isinstance(last_exception, HTTPException):
        raise last_exception
    else:
        raise HTTPException(status_code=500, detail=f"æ‰€æœ‰åç«¯è¯·æ±‚å¤±è´¥: {str(last_exception)}")


# ============ å¯åŠ¨äº‹ä»¶ ============

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    # åŠ è½½é…ç½®
    if not config_loader.load():
        logger.warning("é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åˆå§‹åŒ–åç«¯è·¯ç”±å™¨
    init_backend_routers()
    
    # è·å–ä»£ç†é…ç½®
    proxy_config = config_loader.get_proxy_config()
    port = proxy_config.get("port", 11435)
    host = proxy_config.get("host", "0.0.0.0")
    
    logger.info("=" * 60)
    logger.info("ğŸ¤– æ™ºèƒ½ Ollama å¤šæ¨¡å‹è·¯ç”±ä»£ç†")
    logger.info("=" * 60)
    logger.info(f"ğŸ“¡ ä»£ç†æœåŠ¡è¿è¡Œåœ¨: http://{host}:{port}")
    logger.info(f"ğŸ”§ é…ç½®æ–‡ä»¶: config.yaml")
    logger.info(f"ğŸ“Š å·²åŠ è½½æ¨¡å‹ç»„: {len(config_loader.models)} ä¸ª")
    logger.info(f"ğŸ”Œ åç«¯è·¯ç”±å™¨: {len(backend_manager.routers)} ä¸ª")
    
    # æ˜¾ç¤ºå·²é…ç½®çš„æ¨¡å‹
    virtual_models = config_loader.get_all_virtual_models()
    logger.info(f"âœ¨ è™šæ‹Ÿæ¨¡å‹: {len(virtual_models)} ä¸ª")
    
    logger.info("")
    logger.info("ğŸ’¡ è¯·åœ¨ Copilot ä¸­é…ç½® Ollama åœ°å€ä¸ºä¸Šè¿°ä»£ç†åœ°å€")
    logger.info("=" * 60)
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    logger.info("æ­£åœ¨å…³é—­æœåŠ¡...")
    # å…³é—­ClientPoolä¸­çš„æ‰€æœ‰HTTPå®¢æˆ·ç«¯
    from client_pool import client_pool
    await client_pool.close_all()

# åˆ›å»ºFastAPIåº”ç”¨ï¼ˆä½¿ç”¨ lifespan äº‹ä»¶å¤„ç†å™¨ï¼‰
app = FastAPI(title="Smart Ollama Proxy - å¤šæ¨¡å‹è·¯ç”±", lifespan=lifespan)


# ============ APIç«¯ç‚¹ ============

@app.get("/api/tags")
async def get_models(request: Request):
    """è·å–åˆå¹¶çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæœ¬åœ°+è™šæ‹Ÿï¼‰"""
    try:
        # è®°å½•è¯·æ±‚è¯¦ç»†ä¿¡æ¯
        client_host = request.client.host if request.client else "unknown"
        user_agent = request.headers.get("user-agent", "unknown")
        logger.info(f"æ”¶åˆ° /api/tags è¯·æ±‚ - å®¢æˆ·ç«¯: {client_host}, User-Agent: {user_agent}")
        
        combined_models = await model_router.get_combined_models()
        result = {"models": combined_models}
        
        # è®°å½•è¯¦ç»†çš„è¿”å›ä¿¡æ¯
        local_count = sum(1 for m in combined_models if m.get("details", {}).get("format") != "api")
        virtual_count = sum(1 for m in combined_models if m.get("details", {}).get("format") == "api")
        
        logger.info(f"è¿”å› /api/tags: æ€»å…± {len(combined_models)} ä¸ªæ¨¡å‹ (æœ¬åœ°: {local_count}, è™šæ‹Ÿ: {virtual_count})")
        logger.debug(f"/api/tags è¿”å›æ•°æ®ç¤ºä¾‹: {result['models'][:2] if len(result['models']) > 2 else result}")
        
        return result
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿Copilotä¸ä¼šçœ‹åˆ°é”™è¯¯
        return {"models": []}


@app.post("/api/generate")
async def generate(request: OllamaGenerateRequest):
    """Ollamaç”Ÿæˆè¯·æ±‚"""
    import time
    start_time = time.time()
    
    try:
        # è·å–åç«¯é…ç½®ä»¥æ£€æŸ¥æ˜¯å¦è®°å½•å®Œæ•´æµå¼æ•°æ®
        candidates = await get_backend_candidates_for_model(request.model)
        log_full_data = False
        
        if candidates:
            router_name, backend_config, actual_model = candidates[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰
            if backend_config and hasattr(backend_config, 'log_full_stream_data'):
                log_full_data = backend_config.log_full_stream_data
        
        # è®°å½•è¯·æ±‚è¾“å…¥ï¼ˆæ ¹æ®é…ç½®ä¼˜åŒ–æ—¥å¿—ï¼‰
        if not request.stream or log_full_data:
            # éæµå¼è¯·æ±‚æˆ–é…ç½®äº†è®°å½•å®Œæ•´æµå¼æ•°æ®æ—¶è®°å½•è¯¦ç»†ä¿¡æ¯
            logger.debug("=" * 80)
            logger.debug(f"[OLLAMA /api/generate] æ”¶åˆ°è¯·æ±‚")
            logger.debug(f"æ¨¡å‹: {request.model}")
            logger.debug(f"æµå¼: {request.stream}")
            if log_full_data or not request.stream:
                logger.debug(f"Prompt: {request.prompt[:500]}{'...' if len(request.prompt) > 500 else ''}")
                logger.debug(f"å®Œæ•´Prompté•¿åº¦: {len(request.prompt)} å­—ç¬¦")
                if VERBOSE_JSON_LOGGING:
                    logger.debug(f"Options: {json.dumps(request.options, ensure_ascii=False)}")
                else:
                    logger.debug(f"Options: {request.options}")
            logger.debug("-" * 80)
        else:  # æµå¼è¯·æ±‚ä¸”ä¸è®°å½•å®Œæ•´æ•°æ®æ—¶åªè®°å½•åŸºæœ¬ä¿¡æ¯
            logger.info(f"[OLLAMA /api/generate] æ”¶åˆ°æµå¼è¯·æ±‚ï¼Œæ¨¡å‹: {request.model}")
        
        logger.info(f"æ”¶åˆ°ç”Ÿæˆè¯·æ±‚ï¼Œæ¨¡å‹: {request.model}, æµå¼: {request.stream}")
        
        # è·å–åç«¯è·¯ç”±å™¨
        router_start = time.time()
        router_info = await get_backend_router_for_model(request.model)
        router_time = time.time() - router_start
        logger.info(f"è·¯ç”±æŸ¥æ‰¾è€—æ—¶: {router_time:.3f}ç§’")
        
        if not router_info:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ¨¡å‹: {request.model}")
        
        router_name, backend_config, actual_model = router_info
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ thinking èƒ½åŠ›
        support_thinking = False
        model_info = config_loader.get_model_config(request.model)
        if model_info:
            model_config, virtual_model = model_info
            capabilities = model_config.get_model_capabilities(virtual_model)
            if "thinking" in capabilities:
                support_thinking = True
        
        if router_name == "local":
            # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
            ollama_available = await check_ollama_available()
            
            if not ollama_available:
                logger.info(f"Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè·¯ç”±å™¨å¤„ç†æœ¬åœ°æ¨¡å‹è¯·æ±‚: {request.model}")
                # ä½¿ç”¨mockè·¯ç”±å™¨
                router_name = "mock"
            
            # æœ¬åœ°Ollamaè¯·æ±‚
            local_config = config_loader.get_local_ollama_config()
            base_url = local_config.get("base_url", "http://localhost:11434")
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            ollama_data = {
                "model": request.model,
                "prompt": request.prompt,
                "stream": request.stream,
                "options": request.options
            }
            
            logger.debug(f"[OLLAMA /api/generate] å‘é€åˆ°æœ¬åœ°Ollama")
            if VERBOSE_JSON_LOGGING:
                logger.debug(f"è¯·æ±‚æ•°æ®: {json.dumps(ollama_data, ensure_ascii=False, indent=2)}")
            else:
                logger.debug(f"è¯·æ±‚æ•°æ®æ¦‚è¦: æ¨¡å‹={ollama_data['model']}, æµå¼={ollama_data['stream']}, prompté•¿åº¦={len(ollama_data['prompt'])}")
            
            # é€šè¿‡è·¯ç”±å™¨å¤„ç†
            request_start = time.time()
            response = await backend_manager.handle_request(
                router_name,
                actual_model,
                ollama_data,
                request.stream,
                convert_to_ollama=False,  # æœ¬åœ°å“åº”å·²ç»æ˜¯Ollamaæ ¼å¼
                virtual_model=request.model,
                support_thinking=support_thinking
            )
            request_time = time.time() - request_start
            logger.info(f"åç«¯è¯·æ±‚è€—æ—¶: {request_time:.3f}ç§’")
            
            total_time = time.time() - start_time
            logger.info(f"[OLLAMA /api/generate] æ€»è€—æ—¶: {total_time:.3f}ç§’")
            logger.debug("=" * 80)
            
            return response
        else:
            # OpenAIå…¼å®¹åç«¯è¯·æ±‚
            # å‡†å¤‡OpenAIæ ¼å¼è¯·æ±‚æ•°æ®
            openai_data = {
                "messages": [{"role": "user", "content": request.prompt}],
                "stream": request.stream,
                "temperature": request.options.get("temperature", 0.7),
                "max_tokens": request.options.get("num_predict", 2048),
            }
            
            logger.debug(f"[OLLAMA /api/generate] è½¬æ¢ä¸ºOpenAIæ ¼å¼å¹¶å‘é€åˆ°åç«¯")
            logger.debug(f"è·¯ç”±å™¨: {router_name}, å®é™…æ¨¡å‹: {actual_model}")
            if VERBOSE_JSON_LOGGING:
                logger.debug(f"è¯·æ±‚æ•°æ®: {json.dumps(openai_data, ensure_ascii=False, indent=2)}")
            else:
                logger.debug(f"è¯·æ±‚æ•°æ®æ¦‚è¦: æ¶ˆæ¯æ•°={len(openai_data.get('messages', []))}, æµå¼={openai_data.get('stream', False)}")
            
            # é€šè¿‡åç«¯è·¯ç”±å™¨å¤„ç†
            request_start = time.time()
            response = await backend_manager.handle_request(
                router_name,
                actual_model,
                openai_data,
                request.stream,
                convert_to_ollama=(not request.stream),  # éæµå¼éœ€è¦è½¬æ¢
                virtual_model=request.model,
                support_thinking=support_thinking
            )
            request_time = time.time() - request_start
            logger.info(f"åç«¯è¯·æ±‚è€—æ—¶: {request_time:.3f}ç§’")
            
            total_time = time.time() - start_time
            logger.info(f"[OLLAMA /api/generate] æ€»è€—æ—¶: {total_time:.3f}ç§’")
            logger.debug("=" * 80)
            
            return response
            
    except Exception as e:
        total_time = time.time() - start_time if 'start_time' in locals() else 0
        logger.error(f"å¤„ç†ç”Ÿæˆè¯·æ±‚å¤±è´¥: {e} (è€—æ—¶: {total_time:.3f}ç§’)")
        logger.debug("=" * 80)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAIå…¼å®¹èŠå¤©å®Œæˆç«¯ç‚¹"""
    import time
    start_time = time.time()
    
    try:
        # è·å–è¯·æ±‚ä½“ï¼Œå¤„ç†å¯èƒ½çš„ Unicode ç¼–ç é—®é¢˜
        try:
            body = await request.json()
        except Exception as e:
            # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•è¯»å–åŸå§‹æ•°æ®å¹¶æ¸…ç†
            logger.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†: {e}")
            raw_body = await request.body()
            try:
                # å°è¯•ä½¿ç”¨ UTF-8 è§£ç ï¼Œæ›¿æ¢æ— æ•ˆå­—ç¬¦
                cleaned_body = raw_body.decode('utf-8', errors='replace')
                body = json.loads(cleaned_body)
            except Exception as e2:
                logger.error(f"æ— æ³•è§£æè¯·æ±‚ä½“: {e2}")
                raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„ JSON è¯·æ±‚: {str(e2)}")
        
        model_name = body.get("model", "")
        stream = body.get("stream", False)
        messages = body.get("messages", [])
        
        # è·å–åç«¯é…ç½®ä»¥æ£€æŸ¥æ˜¯å¦è®°å½•å®Œæ•´æµå¼æ•°æ®
        router_info = await get_backend_router_for_model(model_name)
        log_full_data = False
        
        if router_info:
            router_name, backend_config, actual_model = router_info
            if backend_config and hasattr(backend_config, 'log_full_stream_data'):
                log_full_data = backend_config.log_full_stream_data
        
        # è®°å½•è¯·æ±‚è¾“å…¥ï¼ˆæ ¹æ®é…ç½®ä¼˜åŒ–æ—¥å¿—ï¼‰
        if not stream or log_full_data:
            # éæµå¼è¯·æ±‚æˆ–é…ç½®äº†è®°å½•å®Œæ•´æµå¼æ•°æ®æ—¶è®°å½•è¯¦ç»†ä¿¡æ¯
            logger.debug("=" * 80)
            logger.debug(f"[OPENAI /v1/chat/completions] æ”¶åˆ°è¯·æ±‚")
            logger.debug(f"æ¨¡å‹: {model_name}")
            logger.debug(f"æµå¼: {stream}")
            logger.debug(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            if log_full_data or not stream:
                # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆæˆªæ–­é•¿æ¶ˆæ¯ï¼‰
                for i, msg in enumerate(messages):
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    logger.debug(f"æ¶ˆæ¯[{i}] - Role: {role}, Contenté•¿åº¦: {len(content)}, é¢„è§ˆ: {content_preview}")
                
                if VERBOSE_JSON_LOGGING:
                    logger.debug(f"å®Œæ•´è¯·æ±‚ä½“: {json.dumps(body, ensure_ascii=False, indent=2)}")
                else:
                    logger.debug(f"è¯·æ±‚ä½“æ¦‚è¦: æ¨¡å‹={model_name}, æµå¼={stream}, æ¶ˆæ¯æ•°={len(messages)}")
            
            logger.debug("-" * 80)
        else:
            # æµå¼è¯·æ±‚ä¸”ä¸è®°å½•å®Œæ•´æ•°æ®æ—¶åªè®°å½•åŸºæœ¬ä¿¡æ¯
            logger.info(f"[OPENAI /v1/chat/completions] æ”¶åˆ°æµå¼è¯·æ±‚ï¼Œæ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        logger.info(f"æ”¶åˆ°OpenAIèŠå¤©è¯·æ±‚ï¼Œæ¨¡å‹: {model_name}, æµå¼: {stream}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        # è·å–åç«¯è·¯ç”±å™¨
        router_start = time.time()
        router_info = await get_backend_router_for_model(model_name)
        router_time = time.time() - router_start
        logger.info(f"è·¯ç”±æŸ¥æ‰¾è€—æ—¶: {router_time:.3f}ç§’")
        
        if not router_info:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ¨¡å‹: {model_name}")
        
        router_name, backend_config, actual_model = router_info
        
        # æ‰“å°èŠå¤©è¾“å‡ºä¿¡æ¯
        logger.info(f"OpenAIèŠå¤©è·¯ç”±ä¿¡æ¯ - æ¨¡å‹: {model_name}, è·¯ç”±å™¨: {router_name}, å®é™…æ¨¡å‹: {actual_model}")
        if backend_config:
            logger.info(f"åç«¯é…ç½® - URL: {backend_config.base_url}, è¶…æ—¶: {backend_config.timeout}")
            logger.debug(f"åç«¯é…ç½®è¯¦æƒ…: base_url={backend_config.base_url}, timeout={backend_config.timeout}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ thinking èƒ½åŠ›
        support_thinking = False
        model_info = config_loader.get_model_config(model_name)
        if model_info:
            model_config, virtual_model = model_info
            capabilities = model_config.get_model_capabilities(virtual_model)
            if "thinking" in capabilities:
                support_thinking = True

        logger.debug(f"[OPENAI /v1/chat/completions] å‘é€åˆ°åç«¯è·¯ç”±å™¨")
        logger.debug(f"è·¯ç”±å™¨: {router_name}, å®é™…æ¨¡å‹: {actual_model}, support_thinking: {support_thinking}")

        # æ€§èƒ½ç›‘æ§ï¼šè½¬å‘å‰è€—æ—¶ï¼ˆä»æ¥æ”¶åˆ°è¯·æ±‚åˆ°è½¬å‘å‰ï¼‰
        pre_forward_time = time.time() - start_time
        logger.info(f"[OPENAI /v1/chat/completions] è½¬å‘å‰è€—æ—¶: {pre_forward_time:.3f}ç§’")

        # é€šè¿‡åç«¯è·¯ç”±å™¨å¤„ç†
        forward_start = time.time()
        logger.info(f"[OPENAI /v1/chat/completions] å¼€å§‹è½¬å‘åˆ°åç«¯")
        response = await backend_manager.handle_request(
            router_name,
            actual_model,
            body,
            stream,
            convert_to_ollama=False,  # OpenAIç«¯ç‚¹ä¸éœ€è¦è½¬æ¢
            virtual_model=model_name,
            support_thinking=support_thinking
        )
        forward_time = time.time() - forward_start
        logger.info(f"[OPENAI /v1/chat/completions] åç«¯è½¬å‘è€—æ—¶: {forward_time:.3f}ç§’")
        
        # è®°å½•å“åº”ï¼ˆå¦‚æœæ˜¯éæµå¼å“åº”ï¼‰
        if not stream and hasattr(response, 'body'):
            try:
                if isinstance(response.body, bytes):
                    response_data = json.loads(response.body.decode())
                    logger.debug(f"[OPENAI /v1/chat/completions] å“åº”æ•°æ®:")
                    if VERBOSE_JSON_LOGGING:
                        logger.debug(f"{json.dumps(response_data, ensure_ascii=False, indent=2)}")
                    else:
                        # åªæ‰“å°å…³é”®ä¿¡æ¯
                        choices = response_data.get('choices', [])
                        if choices:
                            first_choice = choices[0]
                            message = first_choice.get('message', {})
                            content = message.get('content', '')
                            finish_reason = first_choice.get('finish_reason', 'unknown')
                            logger.debug(f"å“åº”æ¦‚è¦: é€‰æ‹©æ•°={len(choices)}, å†…å®¹é•¿åº¦={len(content)}, å®ŒæˆåŸå› ={finish_reason}")
                        else:
                            logger.debug(f"å“åº”æ¦‚è¦: æ— é€‰æ‹©æ•°æ®")
            except:
                logger.debug(f"[OPENAI /v1/chat/completions] æ— æ³•è§£æå“åº”æ•°æ®")
        
        total_time = time.time() - start_time
        logger.info(f"[OPENAI /v1/chat/completions] æ€»è€—æ—¶: {total_time:.3f}ç§’")
        logger.debug("=" * 80)
        
        return response
            
    except Exception as e:
        total_time = time.time() - start_time if 'start_time' in locals() else 0
        logger.error(f"å¤„ç†OpenAIèŠå¤©è¯·æ±‚å¤±è´¥: {e} (è€—æ—¶: {total_time:.3f}ç§’)")
        logger.debug("=" * 80)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/version")
async def get_version():
    """è·å–ç‰ˆæœ¬ä¿¡æ¯"""
    local_config = config_loader.get_local_ollama_config()
    base_url = local_config.get("base_url", "http://localhost:11434")
    
    # ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œå¿«é€Ÿå¤±è´¥
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            resp = await client.get(f"{base_url}/api/version")
            if resp.status_code == 200:
                return resp.json()
            else:
                logger.warning(f"è·å–Ollamaç‰ˆæœ¬å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
                # è¿”å›æ¨¡æ‹Ÿç‰ˆæœ¬
                return {"version": "0.6.4", "mock": True, "message": "Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç‰ˆæœ¬"}
    except (httpx.ConnectError, httpx.TimeoutException, httpx.ReadTimeout, httpx.ConnectTimeout) as e:
        logger.warning(f"è¿æ¥Ollamaå¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿç‰ˆæœ¬: {type(e).__name__}")
        # è¿”å›æ¨¡æ‹Ÿç‰ˆæœ¬
        return {"version": "0.6.4", "mock": True, "message": "Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç‰ˆæœ¬"}
    except Exception as e:
        logger.warning(f"è·å–Ollamaç‰ˆæœ¬å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿç‰ˆæœ¬: {type(e).__name__}")
        # è¿”å›æ¨¡æ‹Ÿç‰ˆæœ¬
        return {"version": "0.6.4", "mock": True, "message": "Ollamaä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç‰ˆæœ¬"}


@app.post("/api/show")
async def show_model(request: Request):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    try:
        body = await request.json()
        model_name = body.get("model", "")
        
        logger.info(f"æ”¶åˆ°æ¨¡å‹ä¿¡æ¯è¯·æ±‚: {model_name}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè™šæ‹Ÿæ¨¡å‹
        model_info = config_loader.get_model_config(model_name)
        if model_info:
            model_config, virtual_model = model_info
            
            # å¦‚æœæ˜¯è™šæ‹Ÿæ¨¡å‹ï¼Œè¿”å›è™šæ‹Ÿæ¨¡å‹ä¿¡æ¯
            if model_config.model_group != "local":
                # æ„å»ºå®Œæ•´çš„æ¨¡å‹åï¼ˆå¸¦ç»„åï¼‰
                full_model_name = f"{model_config.model_group}/{virtual_model}" if '/' not in model_name else model_name
                logger.info(f"è¿”å›è™šæ‹Ÿæ¨¡å‹ä¿¡æ¯: {full_model_name} (ç»„: {model_config.model_group}, è™šæ‹Ÿæ¨¡å‹: {virtual_model})")
                
                # è·å–åç«¯é…ç½®ä»¥å¡«å……remote_hostå’Œremote_model
                remote_host = ""
                remote_model = virtual_model
                backend_mode = config_loader.routing_config.get("default_backend_mode", "openai_backend")
                backend = model_config.get_backend(backend_mode)
                if backend:
                    remote_host = backend.base_url
                    actual_model = model_config.get_actual_model(virtual_model, backend_mode)
                    if actual_model:
                        remote_model = actual_model
                
                # æ„å»ºæ¨¡å‹ä¿¡æ¯ï¼ˆæ¨¡ä»¿äº‘æ¨¡å‹ç»“æ„ï¼‰
                model_info = {
                    "general.architecture": model_config.model_group,
                    "general.basename": remote_model,
                    f"{model_config.model_group}.context_length": model_config.get_model_context_length(virtual_model),
                    f"{model_config.model_group}.embedding_length": model_config.get_model_embedding_length(virtual_model)
                }
                
                # èƒ½åŠ›åˆ—è¡¨
                capabilities = model_config.get_model_capabilities(virtual_model)
                
                return {
                    "model": full_model_name,  # è¿”å›å¸¦ç»„åçš„å®Œæ•´æ¨¡å‹å
                    "details": {
                        "parent_model": "",
                        "format": "api",
                        "family": model_config.model_group,
                        "families": [model_config.model_group],
                        "parameter_size": "7B",
                        "quantization_level": "FP8_E4M3"
                    },
                    "modelfile": f"# Virtual {model_config.model_group} model via API\nFROM api:{model_config.model_group}\n\nSYSTEM \"You are a helpful AI assistant.\"",
                    "template": "{{ .Prompt }}",
                    "parameters": "num_ctx 4096\nnum_predict 2048\ntemperature 0.7",
                    "license": "",
                    "system": "You are a helpful AI assistant.",
                    "remote_model": remote_model,
                    "remote_host": remote_host,
                    "model_info": model_info,
                    "capabilities": capabilities,
                    "modified_at": "2026-01-14T05:40:00.000000+08:00"
                }
            else:
                logger.info(f"æ¨¡å‹ {model_name} å±äºæœ¬åœ°ç»„ï¼Œè½¬å‘åˆ°æœ¬åœ°Ollama")
        else:
            logger.info(f"æ¨¡å‹ {model_name} æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°ï¼Œå°è¯•ä½œä¸ºæœ¬åœ°æ¨¡å‹å¤„ç†")
        
        # è½¬å‘åˆ°æœ¬åœ°Ollamaï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ¨¡æ‹Ÿå“åº”
        local_config = config_loader.get_local_ollama_config()
        base_url = local_config.get("base_url", "http://localhost:11434")
        
        try:
            logger.info(f"è½¬å‘ /api/show è¯·æ±‚åˆ°æœ¬åœ°Ollama: {base_url}/api/show")
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(f"{base_url}/api/show", json=body)
                logger.info(f"æœ¬åœ°Ollama /api/show å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    response_data = response.json()
                    logger.debug(f"æœ¬åœ°Ollamaè¿”å›çš„æ¨¡å‹ä¿¡æ¯: {response_data.get('model', 'unknown')}")
                    return response_data
                else:
                    error_text = await response.aread() if response.content else "æ— å“åº”å†…å®¹"
                    logger.warning(f"æœ¬åœ°Ollama /api/show è¿”å›é”™è¯¯: {response.status_code}, {error_text[:100]}")
        except Exception as e:
            logger.warning(f"è¿æ¥æœ¬åœ°Ollamaå¤±è´¥: {type(e).__name__}: {e}")
        
        # è¿”å›æ¨¡æ‹Ÿå“åº”
        logger.info(f"ä¸ºæ¨¡å‹ {model_name} è¿”å›æ¨¡æ‹Ÿå“åº”")
        return {
            "model": model_name,
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "unknown",
                "families": ["unknown"],
                "parameter_size": "unknown",
                "quantization_level": "unknown"
            },
            "modelfile": "",
            "template": "",
            "parameters": {},
            "license": "",
            "system": ""
        }
                
    except Exception as e:
        logger.error(f"å¤„ç†æ¨¡å‹ä¿¡æ¯è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
        return {
            "model": "",
            "details": {},
            "modelfile": "",
            "template": "",
            "parameters": {},
            "license": "",
            "system": ""
        }


@app.api_route("/api/{path:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def proxy_to_ollama(path: str, request: Request):
    """è½¬å‘å…¶ä»–Ollama APIè¯·æ±‚ï¼Œå¦‚æœOllamaä¸å¯ç”¨åˆ™è¿”å›æ¨¡æ‹Ÿå“åº”"""
    # æ’é™¤å·²ç»ç‰¹æ®Šå¤„ç†çš„ç«¯ç‚¹
    if path in ["tags", "generate", "version", "show"]:
        # è¿™äº›ç«¯ç‚¹å·²ç»æœ‰ç‰¹æ®Šå¤„ç†
        pass
    
    local_config = config_loader.get_local_ollama_config()
    base_url = local_config.get("base_url", "http://localhost:11434")
    target_url = f"{base_url}/api/{path}"
    
    logger.info(f"è½¬å‘è¯·æ±‚ {request.method} /api/{path} -> {target_url}")
    
    # è·å–è¯·æ±‚ä½“
    body = None
    if request.method in ["POST", "PUT"]:
        body = await request.body()
    
    # è½¬å‘è¯·æ±‚ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ¨¡æ‹Ÿå“åº”
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.request(
                method=request.method,
                url=target_url,
                content=body,
                headers=dict(request.headers),
                params=dict(request.query_params)
            )
            
            logger.info(f"è½¬å‘æˆåŠŸ {request.method} /api/{path} -> çŠ¶æ€ç : {response.status_code}")
            
            # è¿”å›å“åº”
            return JSONResponse(
                content=response.json() if response.content else None,
                status_code=response.status_code,
                headers=dict(response.headers)
            )
    except Exception as e:
        logger.warning(f"è½¬å‘å¤±è´¥ {request.method} /api/{path} -> è¿”å›æ¨¡æ‹Ÿå“åº”: {str(e)}")
        
        # æ ¹æ®è·¯å¾„è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if path == "pull":
            # æ¨¡æ‹Ÿæ‹‰å–æ¨¡å‹å“åº”
            return JSONResponse(
                content={"status": "success", "message": "Model pull simulated (Ollama not available)"},
                status_code=200
            )
        elif path == "delete":
            # æ¨¡æ‹Ÿåˆ é™¤æ¨¡å‹å“åº”
            return JSONResponse(
                content={"status": "success", "message": "Model delete simulated (Ollama not available)"},
                status_code=200
            )
        elif path == "copy":
            # æ¨¡æ‹Ÿå¤åˆ¶æ¨¡å‹å“åº”
            return JSONResponse(
                content={"status": "success", "message": "Model copy simulated (Ollama not available)"},
                status_code=200
            )
        else:
            # é€šç”¨æ¨¡æ‹Ÿå“åº”
            return JSONResponse(
                content={
                    "error": f"Ollama not available: {path}",
                    "mock": True,
                    "message": "Ollama is not available, this is a simulated response"
                },
                status_code=200
            )


@app.get("/api/client-pool")
async def get_client_pool_status():
    """è·å–ClientPoolçŠ¶æ€ä¿¡æ¯"""
    from client_pool import client_pool
    stats = client_pool.get_stats()
    
    return {
        "message": "ClientPoolçŠ¶æ€",
        "stats": stats,
        "description": "HTTPå®¢æˆ·ç«¯æ± ç®¡ç†å™¨ï¼Œç”¨äºå¤ç”¨ç›¸åŒåç«¯é…ç½®çš„HTTPå®¢æˆ·ç«¯"
    }


@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹ï¼Œæ˜¾ç¤ºæœåŠ¡ä¿¡æ¯"""
    proxy_config = config_loader.get_proxy_config()
    local_config = config_loader.get_local_ollama_config()
    
    # è·å–æ¨¡å‹ç»„ä¿¡æ¯
    model_groups = list(config_loader.models.keys())
    backend_routers = list(backend_manager.routers.keys())
    
    return {
        "message": "Smart Ollama Proxy - å¤šæ¨¡å‹è·¯ç”±ä»£ç†",
        "version": "2.0.0",
        "architecture": "æ¨¡å‹é€‰æ‹© -> åç«¯æ¨¡å¼ -> æ¨¡å‹åç«¯",
        "endpoints": {
            "GET /api/tags": "è·å–åˆå¹¶åçš„æ¨¡å‹åˆ—è¡¨",
            "POST /api/generate": "æ™ºèƒ½è·¯ç”±ç”Ÿæˆè¯·æ±‚",
            "POST /v1/chat/completions": "OpenAIå…¼å®¹èŠå¤©å®Œæˆ",
            "GET /api/version": "è·å–ç‰ˆæœ¬ä¿¡æ¯",
            "POST /api/show": "è·å–æ¨¡å‹ä¿¡æ¯",
            "ANY /api/{path}": "è½¬å‘å…¶ä»–Ollama APIè¯·æ±‚"
        },
        "config": {
            "proxy_port": proxy_config.get("port", 11435),
            "local_ollama": local_config.get("base_url", "http://localhost:11434"),
            "model_groups": model_groups,
            "backend_routers": backend_routers
        }
    }


# ============ ä¸»ç¨‹åº ============

if __name__ == "__main__":
    import uvicorn
    
    # åŠ è½½é…ç½®
    if not config_loader.load():
        logger.warning("é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åˆå§‹åŒ–åç«¯è·¯ç”±å™¨
    init_backend_routers()
    
    proxy_config = config_loader.get_proxy_config()
    port = proxy_config.get("port", 11435)
    host = proxy_config.get("host", "0.0.0.0")
    
    uvicorn.run(app, host=host, port=port)