# ğŸ¤– Smart Ollama Proxy - æ™ºèƒ½å¤šæ¨¡å‹è·¯ç”±ä»£ç†

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‘ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [Docker ä½¿ç”¨](#docker-ä½¿ç”¨)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [API æ¥å£](#api-æ¥å£)
- [æ‰©å±•æ€§](#æ‰©å±•æ€§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [é¡¹ç›®æ–‡ä»¶ç»“æ„](#é¡¹ç›®æ–‡ä»¶ç»“æ„)
- [æ”¯æŒçš„æ¨¡å‹æä¾›å•†](#æ”¯æŒçš„æ¨¡å‹æä¾›å•†)
- [è®¸å¯è¯](#è®¸å¯è¯)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [è‡´è°¢](#è‡´è°¢)

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

Smart Ollama Proxy æ˜¯ä¸€ä¸ªæ™ºèƒ½è·¯ç”±ä»£ç†ï¼Œä¸º GitHub Copilot å’Œå…¶ä»– AI å®¢æˆ·ç«¯æä¾›ç»Ÿä¸€çš„æ¨¡å‹è®¿é—®æ¥å£ã€‚å®ƒèƒ½å¤Ÿå°† Ollama API è¯·æ±‚æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒçš„ AI æ¨¡å‹åç«¯ï¼ŒåŒ…æ‹¬æœ¬åœ° Ollama æ¨¡å‹å’Œå¤šç§äº‘ç«¯ AI APIï¼ˆDeepSeekã€OpenAIã€Claudeã€Groq ç­‰ï¼‰ã€‚

é€šè¿‡è¿™ä¸ªä»£ç†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ GitHub Copilotã€Cursor æˆ–å…¶ä»–æ”¯æŒ Ollama åè®®çš„å®¢æˆ·ç«¯æ— ç¼è®¿é—®æ•°åç§ä¸åŒçš„ AI æ¨¡å‹ï¼Œè€Œæ— éœ€ä¿®æ”¹å®¢æˆ·ç«¯é…ç½®ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”Œ å¤šæ¨¡å‹æ”¯æŒ**: æœ¬åœ° Ollama æ¨¡å‹ + äº‘ç«¯ APIï¼ˆDeepSeekã€OpenAIã€Claudeã€Groq ç­‰ï¼‰
- **âš™ï¸ æ™ºèƒ½è·¯ç”±**: æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨è·¯ç”±åˆ°åˆé€‚çš„åç«¯
- **ğŸ”§ çµæ´»é…ç½®**: YAML é…ç½®ï¼Œæ¨¡å‹åˆ†ç»„ç®¡ç†
- **ğŸ¯ å®Œå…¨å…¼å®¹**: åŸç”Ÿæ”¯æŒ Ollama REST API å’Œ OpenAI å…¼å®¹ API
- **ğŸš€ ç”Ÿäº§å°±ç»ª**: å¼‚æ­¥ FastAPI æ¡†æ¶ï¼Œä¼˜é›…çš„é”™è¯¯å¤„ç†ï¼ŒWindows/Linux/macOS æ”¯æŒ
 - **ğŸ¤– GitHub Copilot é›†æˆ**: æ— ç¼é›†æˆï¼Œæ”¯æŒæ‰€æœ‰æ¨¡å‹
 - **ğŸ”§ LiteLLM é›†æˆ**: å¯é€‰é›†æˆ LiteLLM SDKï¼Œè·å¾—é‡è¯•ã€å›é€€ã€æˆæœ¬è·Ÿè¸ªç­‰é«˜çº§åŠŸèƒ½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç³»ç»Ÿè¦æ±‚
- **Python**: 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬
- **æ“ä½œç³»ç»Ÿ**: Windows / Linux / macOS
- **ç½‘ç»œ**: å¯è®¿é—®äº’è”ç½‘ï¼ˆç”¨äºäº‘ç«¯ APIï¼‰

### 1. å…‹éš†æˆ–ä¸‹è½½é¡¹ç›®
```bash
git clone <repository-url>
cd smart_ollama_proxy
```

### 2. å®‰è£…ä¾èµ–

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ pipï¼ˆæ¨èï¼‰
```bash
pip install -r requirements.txt
```

#### æ–¹å¼äºŒï¼šæ‰‹åŠ¨å®‰è£…
```bash
pip install fastapi uvicorn httpx pydantic pyyaml
```

### 3. é…ç½® API å¯†é’¥

ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼Œå°†å„åç«¯çš„ `api_key` æ›¿æ¢ä¸ºå®é™…çš„ API å¯†é’¥ï¼š

```yaml
models:
  deepseek:
    openai_backend:
      api_key: "sk-your-deepseek-api-key-here"
  
  openai:
    openai_backend:
      api_key: "sk-your-openai-api-key-here"
  
  claude:
    openai_backend:
      api_key: "sk-your-claude-api-key-here"
  
  groq:
    openai_backend:
      api_key: "sk-your-groq-api-key-here"
```

> **æ³¨æ„**: å¦‚æœæ‚¨åªä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ï¼Œå¯ä»¥è·³è¿‡ API å¯†é’¥é…ç½®ã€‚

### 4. å¯åŠ¨ä»£ç†æœåŠ¡

**Windows ç”¨æˆ·:**
```bash
run.bat
```

**å…¶ä»–ç³»ç»Ÿ:**
```bash
python main.py
# æˆ–ç”Ÿäº§ç¯å¢ƒä½¿ç”¨
uvicorn main:app --host 0.0.0.0 --port 11435 --reload
```

### 5. é…ç½® GitHub Copilot

1. æ‰“å¼€ GitHub Copilot è®¾ç½®
2. è¿›å…¥ "Advanced" æˆ– "ä»£ç†è®¾ç½®"
3. å°† Ollama åœ°å€è®¾ç½®ä¸º `http://localhost:11435`
4. ä¿å­˜è®¾ç½®å¹¶é‡æ–°å¯åŠ¨ Copilot

### 6. éªŒè¯å®‰è£…

è®¿é—®ä»¥ä¸‹åœ°å€éªŒè¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼š
- æœåŠ¡ä¸»é¡µ: http://localhost:11435
- æ¨¡å‹åˆ—è¡¨: http://localhost:11435/api/tags
- API æ–‡æ¡£: http://localhost:11435/docs

## ğŸ³ Docker ä½¿ç”¨

é¡¹ç›®æä¾›å®˜æ–¹ Docker é•œåƒï¼Œæ–¹ä¾¿åœ¨å®¹å™¨ç¯å¢ƒä¸­å¿«é€Ÿéƒ¨ç½²ã€‚

```bash
# æ‹‰å–æœ€æ–°é•œåƒï¼ˆè¯·æ ¹æ®å®é™…é•œåƒä»“åº“åœ°å€æ›¿æ¢ï¼‰
docker pull ghcr.io/yourorg/smart-ollama-proxy:latest

# è¿è¡Œå®¹å™¨
docker run -d \
  -p 11435:11435 \
  -v $(pwd)/config.yaml:/app/config.yaml \
  ghcr.io/yourorg/smart-ollama-proxy:latest
```

### ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

| å˜é‡å | è¯´æ˜ |
|---|---|
| `PROXY_HOST` | ä»£ç†ç»‘å®šçš„ä¸»æœºåœ°å€ï¼Œé»˜è®¤ `0.0.0.0` |
| `PROXY_PORT` | ä»£ç†ç›‘å¬ç«¯å£ï¼Œé»˜è®¤ `11435` |
| `LOG_LEVEL` | æ—¥å¿—çº§åˆ«ï¼Œ`DEBUG`/`INFO`/`WARNING`/`ERROR`ï¼Œé»˜è®¤ `INFO` |

å®¹å™¨å¯åŠ¨æ—¶å¯é€šè¿‡ `-e` å‚æ•°ä¼ å…¥ï¼Œä¾‹å¦‚ï¼š

```bash
docker run -d -p 11435:11435 \
  -e LOG_LEVEL=DEBUG \
  -v $(pwd)/config.yaml:/app/config.yaml \
  ghcr.io/yourorg/smart-ollama-proxy:latest
```

å¦‚éœ€è‡ªå®šä¹‰æ›´å¤šé…ç½®ï¼Œè¯·å‚è€ƒåé¢çš„ **é…ç½®è¯´æ˜** éƒ¨åˆ†ã€‚

## âš™ï¸ é…ç½®è¯´æ˜

Smart Ollama Proxy ä½¿ç”¨ YAML æ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼ˆ`config.yaml`ï¼‰ï¼Œä»¥ä¸‹æ˜¯å…³é”®é…ç½®é¡¹ï¼š

### åŸºç¡€é…ç½®
```yaml
proxy:
  port: 11435              # ä»£ç†æœåŠ¡ç«¯å£
  host: "0.0.0.0"          # ç»‘å®šåœ°å€
  log_level: "INFO"        # æ—¥å¿—çº§åˆ«
  # æ˜¯å¦å¯ç”¨è¯¦ç»†çš„JSONæ—¥å¿—è®°å½•ï¼ˆä¼šæ‰“å°å®Œæ•´çš„è¯·æ±‚/å“åº”JSONæ•°æ®ï¼‰
  verbose_json_logging: false
  # æ˜¯å¦å¯ç”¨å·¥å…·å‹ç¼©ä¼˜åŒ–ï¼ˆæ£€æµ‹é‡å¤å·¥å…·åˆ—è¡¨å¹¶å‹ç¼©ï¼‰
  tool_compression_enabled: true
  # æ˜¯å¦å¯ç”¨é‡å¤æç¤ºè¯å‹ç¼©ä¼˜åŒ–ï¼ˆä»å†…å®¹å¤´å¼€å§‹æ¯”å¯¹ä¸ä¸Šæ¬¡å†…å®¹ï¼Œå°†é‡å¤éƒ¨åˆ†æ›¿æ¢ä¸ºæ ‡è®°ï¼‰
  prompt_compression_enabled: true
  # æ˜¯å¦å¯ç”¨HTTPä¼ è¾“å‹ç¼©ï¼ˆgzip/deflateï¼‰
  http_compression_enabled: true

local_ollama:
  base_url: "http://localhost:11434"  # æœ¬åœ° Ollama æœåŠ¡åœ°å€
```

### HTTPå‹ç¼©é…ç½®
Smart Ollama Proxy æ”¯æŒ HTTP ä¼ è¾“å‹ç¼©ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘ç½‘ç»œä¼ è¾“æ•°æ®é‡ï¼Œæé«˜å›½é™… API è°ƒç”¨çš„é€Ÿåº¦ã€‚

**å…¨å±€é…ç½®**ï¼šé€šè¿‡ `proxy.http_compression_enabled` æ§åˆ¶æ˜¯å¦å¯ç”¨ HTTP å‹ç¼©ï¼ˆé»˜è®¤å¯ç”¨ï¼‰ã€‚å¯ç”¨åï¼Œä»£ç†ä¼šåœ¨ HTTP è¯·æ±‚ä¸­æ·»åŠ  `Accept-Encoding: gzip, deflate, br` å¤´ï¼Œå¹¶è‡ªåŠ¨å¤„ç†æœåŠ¡å™¨çš„å‹ç¼©å“åº”ã€‚

**åç«¯çº§é…ç½®**ï¼šæ¯ä¸ªåç«¯å¯ä»¥å•ç‹¬é…ç½® `compression_enabled` é€‰é¡¹ï¼ˆé»˜è®¤ç»§æ‰¿å…¨å±€è®¾ç½®ï¼‰ï¼š
```yaml
models:
  deepseek:
    openai_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      timeout: 30
      compression_enabled: true  # æ˜¯å¦å¯ç”¨HTTPå‹ç¼©ï¼ˆé»˜è®¤trueï¼Œç»§æ‰¿å…¨å±€proxy.http_compression_enabledï¼‰
```

**æ³¨æ„äº‹é¡¹**ï¼š
- å¤§å¤šæ•° AI APIï¼ˆOpenAIã€DeepSeekã€Anthropicã€Groq ç­‰ï¼‰éƒ½æ”¯æŒ gzip å‹ç¼©
- æœ¬åœ° Ollama æœåŠ¡é€šå¸¸ä¸æ”¯æŒå‹ç¼©ï¼Œä½†å¯ç”¨å‹ç¼©ä¸ä¼šå¯¼è‡´é”™è¯¯
- å‹ç¼©å¯ä»¥æ˜¾è‘—å‡å°‘å“åº”ä½“ç§¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸‹
- ç›‘æ§æ—¥å¿—ä¸­ä¼šæ˜¾ç¤ºå®¢æˆ·ç«¯å‹ç¼©å¯ç”¨çŠ¶æ€ï¼ˆDEBUG çº§åˆ«ï¼‰

### API å¯†é’¥é…ç½®
```yaml
models:
  deepseek:
    openai_backend:
      api_key: "sk-your-deepseek-api-key"
  
  openai:
    openai_backend:
      api_key: "sk-your-openai-api-key"
  
   # å…¶ä»–æ¨¡å‹ç»„ç±»ä¼¼é…ç½®
```

### LiteLLM é…ç½®ï¼ˆå¯é€‰ï¼‰
Smart Ollama Proxy æ”¯æŒå¯é€‰é›†æˆ [LiteLLM](https://github.com/BerriAI/litellm) SDKï¼Œæä¾›æ›´é«˜çº§çš„åŠŸèƒ½å¦‚è‡ªåŠ¨é‡è¯•ã€å›é€€ã€æˆæœ¬è·Ÿè¸ªç­‰ã€‚è¦å¯ç”¨ LiteLLMï¼š

1. å®‰è£… LiteLLMï¼š`pip install litellm`
2. åœ¨é…ç½®ä¸­æ·»åŠ  LiteLLM å‚æ•°ï¼š

```yaml
models:
  deepseek:
    openai_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      use_litellm: true  # å¯ç”¨ LiteLLMï¼ˆé»˜è®¤ trueï¼Œå¦‚æœå·²å®‰è£…ï¼‰
      litellm_params:    # LiteLLM ä¸“ç”¨å‚æ•°
        max_retries: 3   # æœ€å¤§é‡è¯•æ¬¡æ•°
        cache: true      # å¯ç”¨ç¼“å­˜
        timeout: 30      # è¶…æ—¶æ—¶é—´
```

**æ³¨æ„**ï¼šå¦‚æœæœªå®‰è£… `litellm` åŒ…ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å›é€€åˆ°æ ‡å‡†çš„ HTTP è¯·æ±‚ï¼Œä¸å½±å“æ­£å¸¸ä½¿ç”¨ã€‚

### LiteLLM ä¸“ç”¨åç«¯é…ç½®
ä» v1.1 å¼€å§‹ï¼ŒSmart Ollama Proxy æ”¯æŒç‹¬ç«‹çš„ `litellm_backend` é…ç½®ï¼Œä¸“é—¨ç”¨äº LiteLLM é›†æˆï¼š

```yaml
models:
  deepseek:
    # OpenAIå…¼å®¹åç«¯ï¼ˆä½¿ç”¨OpenAI SDK + HTTPå›é€€ï¼‰
    openai_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      # backend_type: "openai_sdk"  # å¯é€‰ï¼šopenai_sdk, http, openai (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)
    
    # LiteLLMä¸“ç”¨åç«¯
    litellm_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      max_retries: 3   # LiteLLMä¸“ç”¨å‚æ•°
      cache: true      # å¯ç”¨ç¼“å­˜
      timeout: 30      # è¶…æ—¶æ—¶é—´
```

**ä¸¤ç§é…ç½®æ–¹å¼çš„åŒºåˆ«**ï¼š
1. **`openai_backend` + `use_litellm: true`**ï¼šå…¼å®¹æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨ OpenAI SDKï¼Œå¤±è´¥å›é€€ HTTP
2. **`litellm_backend`**ï¼šä¸“ç”¨æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨ LiteLLM SDK å¤„ç†æ‰€æœ‰è¯·æ±‚

### åç«¯è·¯ç”±å™¨æ¶æ„
Smart Ollama Proxy ä½¿ç”¨æ¨¡å—åŒ–çš„åç«¯è·¯ç”±å™¨æ¶æ„ï¼š

| åç«¯ç±»å‹ | è·¯ç”±å™¨ç±» | è¯´æ˜ |
|---------|----------|------|
| `openai_backend` | `OpenAIBackendRouter` | ä¼˜å…ˆä½¿ç”¨ OpenAI Python SDKï¼Œå¤±è´¥æ—¶å›é€€åˆ° HTTP è¯·æ±‚ |
| `litellm_backend` | `LiteLLMRouter` | ä¸“é—¨ä½¿ç”¨ LiteLLM SDK å¤„ç†è¯·æ±‚ |
| `openai_sdk` | `OpenAISDKBackendRouter` | ä»…ä½¿ç”¨ OpenAI SDKï¼ˆéœ€è¦æ˜¾å¼é…ç½® `backend_type`ï¼‰ |
| `ollama` | `OllamaBackendRouter` | æœ¬åœ° Ollama æœåŠ¡ |
| `mock` | `MockBackendRouter` | æ¨¡æ‹Ÿåç«¯ï¼Œç”¨äºæµ‹è¯• |

**è‡ªåŠ¨ç±»å‹æ¨æ–­**ï¼šç³»ç»Ÿä¼šæ ¹æ®é…ç½®çš„ `backend_mode`ï¼ˆå¦‚ `openai_backend`ã€`litellm_backend`ï¼‰è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è·¯ç”±å™¨ç±»å‹ã€‚

å®Œæ•´çš„é…ç½®ç¤ºä¾‹è¯·å‚è€ƒ `config.yaml` æ–‡ä»¶ã€‚

## ğŸ“¡ API æ¥å£

Smart Ollama Proxy æä¾›ä¸¤ç§ä¸»è¦çš„ API æ¥å£ï¼š

### ğŸ”Œ Ollama å…¼å®¹ API
å®Œå…¨å…¼å®¹ Ollama åŸç”Ÿ APIï¼Œæ”¯æŒæ‰€æœ‰æ ‡å‡†ç«¯ç‚¹ï¼š

```bash
# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:11435/api/tags

# æ–‡æœ¬ç”Ÿæˆ
curl -X POST http://localhost:11435/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-chat",
    "prompt": "è§£é‡Šä¸€ä¸‹Pythonçš„ç”Ÿæˆå™¨",
    "stream": false
  }'
```

### ğŸ¯ OpenAI å…¼å®¹ API
æä¾› OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£ï¼š

```bash
# èŠå¤©å®Œæˆ
curl -X POST http://localhost:11435/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹"},
      {"role": "user", "content": "è§£é‡ŠJavaScripté—­åŒ…çš„æ¦‚å¿µ"}
    ],
    "stream": true
  }'
```

### æ¨¡å‹å‘½åçº¦å®š

Smart Ollama Proxy æ”¯æŒä¸¤ç§æ¨¡å‹å‘½åæ ¼å¼ï¼š

1. **çº¯æ¨¡å‹å**ï¼š`deepseek-chat`ã€`deepseek-reasoner`ã€`qwen3-max`
2. **å¸¦ç»„åçš„æ¨¡å‹å**ï¼š`deepseek/deepseek-chat`ã€`deepseek/deepseek-reasoner`ã€`qwen/qwen3-max`

ä¸¤ç§æ ¼å¼å®Œå…¨å…¼å®¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†ã€‚å¸¦ç»„åçš„æ ¼å¼æœ‰åŠ©äºæ˜ç¡®æŒ‡å®šæ¨¡å‹ç»„ï¼Œé¿å…æ­§ä¹‰ã€‚

### å¸¸ç”¨æ¨¡å‹ç¤ºä¾‹

| æ¨¡å‹ | ç±»å‹ | API ç«¯ç‚¹ | ç”¨é€” |
|------|------|----------|------|
| `deepseek-chat` æˆ– `deepseek/deepseek-chat` | èŠå¤©æ¨¡å‹ | `/api/chat` æˆ– `/v1/chat/completions` | é€šç”¨å¯¹è¯ã€ä»£ç ç”Ÿæˆ |
| `deepseek-reasoner` æˆ– `deepseek/deepseek-reasoner` | æ¨ç†æ¨¡å‹ | `/api/generate` | å¤æ‚é—®é¢˜æ¨ç† |
| `gpt-4o` | æ™ºèƒ½æ¨¡å‹ | `/v1/chat/completions` | é«˜è´¨é‡å›ç­”ã€ç¼–ç¨‹è¾…åŠ© |
| `claude-3-5-sonnet` | æ™ºèƒ½æ¨¡å‹ | `/v1/chat/completions` | åˆ›æ„å†™ä½œã€åˆ†æ |
| `llama3.2:latest` | æœ¬åœ°æ¨¡å‹ | `/api/generate` | æœ¬åœ°æ¨ç†ã€æµ‹è¯• |
| `llama-3.3-70b` | é«˜é€Ÿæ¨ç† | `/v1/chat/completions` | å¿«é€Ÿå“åº”ã€å¯¹è¯ |

## ğŸ”§ æ‰©å±•æ€§

Smart Ollama Proxy é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°çš„æ¨¡å‹æä¾›å•†ã€‚

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
ç”¨æˆ·è¯·æ±‚ â†’ FastAPI åº”ç”¨ â†’ æ¨¡å‹è·¯ç”±å™¨ â†’ åç«¯è·¯ç”±å™¨ â†’ å®é™… API è°ƒç”¨
```

- **æ¨¡å—åŒ–è®¾è®¡**: é€šè¿‡æ’ä»¶åŒ–æ–¹å¼æ·»åŠ æ–°æ¨¡å‹æä¾›å•†
- **æ ‡å‡†æ¥å£**: ç»Ÿä¸€çš„åç«¯è·¯ç”±å™¨æ¥å£
- **é…ç½®é©±åŠ¨**: é€šè¿‡é…ç½®æ–‡ä»¶è½»æ¾æ·»åŠ æ–°æ¨¡å‹

å½“å‰æ”¯æŒçš„åç«¯ç±»å‹ï¼š
- **openai_backend**: OpenAI å…¼å®¹ APIï¼ˆDeepSeekã€OpenAIã€Claudeã€Groq ç­‰ï¼‰
- **ollama**: æœ¬åœ° Ollama æœåŠ¡
- **mock**: æ¨¡æ‹Ÿåç«¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰

### ğŸ”„ åç«¯ä¼˜å…ˆçº§ä¸å›é€€æœºåˆ¶

Smart Ollama Proxy æ”¯æŒåç«¯ä¼˜å…ˆçº§é…ç½®å’Œè‡ªåŠ¨å›é€€æœºåˆ¶ã€‚å½“æ¨¡å‹ç»„é…ç½®å¤šä¸ªåç«¯æ—¶ï¼Œç³»ç»Ÿä¼šæŒ‰ç…§é…ç½®æ–‡ä»¶ä¸­çš„é¡ºåºå†³å®šä¼˜å…ˆçº§ï¼Œå¦‚æœå‰ä¸€ä¸ªåç«¯å¤±è´¥ä¼šè‡ªåŠ¨å°è¯•ä¸‹ä¸€ä¸ªã€‚

#### é…ç½®ç¤ºä¾‹

```yaml
models:
  deepseek:
    description: "DeepSeek V3.2 ç³»åˆ—æ¨¡å‹"
    available_models:
      deepseek-chat:
        context_length: 128000
        capabilities: ["completion", "tools"]
        actual_model: "deepseek-chat"
    
    # åç«¯é…ç½®ï¼ˆæŒ‰é…ç½®é¡ºåºå†³å®šä¼˜å…ˆçº§ï¼Œå¦‚æœå‰ä¸€ä¸ªå¤±è´¥åˆ™å°è¯•åä¸€ä¸ªï¼‰
    # OpenAIå…¼å®¹åç«¯é…ç½®ï¼ˆä¼˜å…ˆçº§1ï¼‰
    openai_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      timeout: 30
    
    # LiteLLMå…¼å®¹åç«¯é…ç½®ï¼ˆä¼˜å…ˆçº§2ï¼‰
    litellm_backend:
      base_url: "https://api.deepseek.com/v1"
      api_key: "sk-your-deepseek-api-key"
      timeout: 30
      max_retries: 3
      cache: true
```

#### å·¥ä½œåŸç†

1. **ä¼˜å…ˆçº§é¡ºåº**ï¼šYAML é…ç½®æ–‡ä»¶ä¸­åç«¯é…ç½®çš„ä¹¦å†™é¡ºåºå†³å®šä¼˜å…ˆçº§
2. **è‡ªåŠ¨å›é€€**ï¼šå½“è¯·æ±‚å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ã€è®¤è¯å¤±è´¥ã€APIé™æµç­‰ï¼‰æ—¶è‡ªåŠ¨å°è¯•ä¸‹ä¸€ä¸ªåç«¯
3. **è·¯ç”±å™¨å¤ç”¨**ï¼šç›¸åŒé…ç½®çš„åç«¯å…±äº«è·¯ç”±å™¨å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
4. **å…¼å®¹æ€§**ï¼šç°æœ‰ API å®Œå…¨å…¼å®¹ï¼Œå¯é€šè¿‡ `backend_mode` å‚æ•°æŒ‡å®šç‰¹å®šåç«¯

#### æ—¥å¿—è¾“å‡ºç¤ºä¾‹

å½“å¯ç”¨è°ƒè¯•æ—¥å¿—æ—¶ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºå›é€€è¿‡ç¨‹ï¼š

```
å°è¯•åç«¯ 1/2: deepseek.openai_backend
å°è¯•åç«¯ 2/2: deepseek.litellm_backend
âœ… åç«¯ deepseek.litellm_backend è¯·æ±‚æˆåŠŸ
```

#### ä½¿ç”¨å»ºè®®

- **é«˜å¯ç”¨é…ç½®**ï¼šä¸ºå…³é”®æ¨¡å‹é…ç½®å¤šä¸ªåç«¯ï¼Œæé«˜ç³»ç»Ÿå¯ç”¨æ€§
- **ä¼˜å…ˆçº§è§„åˆ’**ï¼šå°†æ€§èƒ½æ›´å¥½ã€æˆæœ¬æ›´ä½çš„åç«¯é…ç½®åœ¨å‰
- **æµ‹è¯•éªŒè¯**ï¼šä½¿ç”¨ `test_priority_fallback.py` æµ‹è¯•ä¼˜å…ˆçº§å’Œå›é€€é€»è¾‘

### ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
smart_ollama_proxy/
â”œâ”€â”€ ğŸ§ª test_api.py              # API æµ‹è¯•è„šæœ¬
â”œâ”€â”€ ğŸ§ª test_mock.py             # æ¨¡æ‹Ÿåç«¯æµ‹è¯•
â”œâ”€â”€ ğŸ§ª test_refactor.py         # é‡æ„æµ‹è¯•è„šæœ¬
â”œâ”€â”€ ğŸ§ª test_priority_fallback.py # åç«¯ä¼˜å…ˆçº§å’Œå›é€€æµ‹è¯•
â””â”€â”€ ğŸ§ª test_litellm_integration.py # LiteLLMé›†æˆæµ‹è¯•
```

## â— å¸¸è§é—®é¢˜

### 1. é…ç½®åŠ è½½å¤±è´¥
- **æ£€æŸ¥ YAML è¯­æ³•**: `python -c "import yaml; yaml.safe_load(open('config.yaml'))"`
- **æ£€æŸ¥ç¼©è¿›æ ¼å¼**: YAML å¯¹ç¼©è¿›è¦æ±‚ä¸¥æ ¼
- **éªŒè¯æ–‡ä»¶ç¼–ç **: ä½¿ç”¨ UTF-8 ç¼–ç 

### 2. API è¯·æ±‚å¤±è´¥
- **æ£€æŸ¥ API å¯†é’¥**: ç¡®ä¿é…ç½®æ­£ç¡®
- **æ£€æŸ¥ç½‘ç»œè¿æ¥**: ç¡®ä¿å¯ä»¥è®¿é—®å¯¹åº”çš„ API æœåŠ¡
- **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® `log_level: "DEBUG"`

### 3. æ¨¡å‹æœªæ‰¾åˆ°
- **æ£€æŸ¥æ¨¡å‹åˆ—è¡¨**: `curl http://localhost:11435/api/tags`
- **éªŒè¯æ¨¡å‹é…ç½®**: æ£€æŸ¥ `config.yaml` ä¸­çš„æ¨¡å‹é…ç½®

### 4. æœ¬åœ° Ollama è¿æ¥å¤±è´¥
- **æ£€æŸ¥ Ollama æœåŠ¡**: è¿è¡Œ `curl http://localhost:11434/api/tags`
- **éªŒè¯é…ç½®**: æ£€æŸ¥ `local_ollama.base_url` é…ç½®
- **å®‰è£… Ollama**: å¯ä» https://ollama.com/ ä¸‹è½½å®‰è£…

### 5. GitHub Copilot è¿æ¥é—®é¢˜
- **æ£€æŸ¥ä»£ç†æœåŠ¡**: `curl http://localhost:11435/`
- **éªŒè¯é…ç½®**: ç¡®ä¿ Copilot è®¾ç½®ä¸­çš„ Ollama åœ°å€ä¸º `http://localhost:11435`
- **æ£€æŸ¥é˜²ç«å¢™**: ç¡®ä¿ç«¯å£ 11435 æœªè¢«é˜»æ­¢

### è°ƒè¯•å»ºè®®
- **å¯ç”¨è°ƒè¯•æ—¥å¿—**: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® `log_level: "DEBUG"`
- **æ£€æŸ¥æœåŠ¡çŠ¶æ€**: `curl http://localhost:11435/health`
- **éªŒè¯æ¨¡å‹åˆ—è¡¨**: `curl http://localhost:11435/api/tags`

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
smart_ollama_proxy/
â”œâ”€â”€ main.py                    # ä¸»åº”ç”¨å…¥å£ï¼ŒFastAPI åº”ç”¨
â”œâ”€â”€ config.yaml               # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ config_loader.py          # é…ç½®åŠ è½½ã€æ¨¡å‹è·¯ç”±
â”œâ”€â”€ backend_router.py         # åç«¯è·¯ç”±å™¨ç³»ç»Ÿ
â”œâ”€â”€ requirements.txt          # Python ä¾èµ–
â”œâ”€â”€ README.md                 # æœ¬æ–‡æ¡£
â”œâ”€â”€ run.bat                   # Windows å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_api.py              # API æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_mock.py             # æ¨¡æ‹Ÿåç«¯æµ‹è¯•
â”œâ”€â”€ test_refactor.py         # é‡æ„æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_priority_fallback.py # åç«¯ä¼˜å…ˆçº§å’Œå›é€€æµ‹è¯•
â””â”€â”€ test_litellm_integration.py # LiteLLMé›†æˆæµ‹è¯•
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹æä¾›å•†

- **DeepSeek**: deepseek-chat, deepseek-reasonerï¼ˆæ”¯æŒ thinking èƒ½åŠ›ï¼‰
- **OpenAI**: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
- **Anthropic**: claude-3-5-sonnet, claude-3-opus
- **Groq**: llama-3.3-70b, mixtral-8x7bï¼ˆé«˜é€Ÿæ¨ç†ï¼‰
- **æœ¬åœ° Ollama**: æ”¯æŒæ‰€æœ‰ Ollama æ¨¡å‹

## ğŸ“œ è®¸å¯è¯

MIT License

```
Copyright (c) 2026 Smart Ollama Proxy Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºé¡¹ç›®åšè´¡çŒ®ï¼š
- æŠ¥å‘Š bug å’Œé—®é¢˜
- æäº¤åŠŸèƒ½è¯·æ±‚
- æäº¤ä»£ç æ”¹è¿›å’Œä¿®å¤
- æ”¹è¿›æ–‡æ¡£

### å¼€å‘è§„èŒƒ
- éµå¾ª PEP 8 ç¼–ç è§„èŒƒ
- ä½¿ç”¨ç±»å‹æ³¨è§£
- ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•
- æ›´æ–°ç›¸å…³æ–‡æ¡£

æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®çš„äººï¼

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œå·¥å…·çš„æ”¯æŒï¼š
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£ã€å¿«é€Ÿçš„ Web æ¡†æ¶
- [Ollama](https://ollama.com/) - æœ¬åœ° AI æ¨¡å‹è¿è¡Œå¹³å°
- [OpenAI API](https://platform.openai.com/) - äº‘ç«¯ AI æ¨¡å‹æœåŠ¡
- [DeepSeek](https://platform.deepseek.com/) - ä¼˜è´¨çš„ AI æ¨¡å‹æä¾›å•†
- [GitHub Copilot](https://github.com/features/copilot) - AI ç¼–ç¨‹åŠ©æ‰‹